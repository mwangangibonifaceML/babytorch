{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9101419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ec8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.optimizers.optim import SGD, Adam, AdamW\n",
    "from minitorch.nn.layers import Linear\n",
    "from minitorch.losses.losses import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "308650d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: SGD Optimizer ....\n",
      "[0.8 1.9] Tensor(data=[0.8 1.9])\n",
      "Basic SGD optimizer works correctly\n",
      "SGD Oprimizer with momentum works correctly \n",
      "SGD Optimizer with weight decay works correctly\n",
      "SGD optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def sgd_unit_test():\n",
    "    \"\"\"Test SGD optimizer implemetation\"\"\"\n",
    "    print('Unit Test: SGD Optimizer ....')\n",
    "    \n",
    "    #* basic optimizer test\n",
    "    param = Tensor(np.array([1.0,2.0], dtype=np.float32), requires_grad=True)\n",
    "    optimizer = SGD([param], lr=0.1)\n",
    "    param.grad = Tensor(np.array([2.0, 1.0], dtype=np.float32))\n",
    "    original_data = param.data.copy()\n",
    "    grad = param.grad.data\n",
    "    optimizer.step()\n",
    "    \n",
    "    expected = original_data - optimizer.learning_rate * grad\n",
    "    print(expected, param)\n",
    "    assert np.allclose(expected, param.data)\n",
    "    assert optimizer.step_count == 1\n",
    "    print('Basic SGD optimizer works correctly')\n",
    "    \n",
    "    # optimizer with momentum test\n",
    "    param2 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_momentum = SGD([param2], lr=0.1, momentum=0.9)\n",
    "    param2.grad = Tensor(np.array([2.0, 1.0]))\n",
    "    original_data = param2.data.copy()\n",
    "    grad = param2.grad.data\n",
    "    optimizer_momentum.step()\n",
    "    \n",
    "    expected = original_data - optimizer_momentum.learning_rate * grad\n",
    "    assert np.allclose(expected.data, param2.data)\n",
    "    assert optimizer_momentum.step_count == 1, f'step count expected to be 1 got {optimizer_momentum.step_count}'\n",
    "    print('SGD Oprimizer with momentum works correctly ')\n",
    "    \n",
    "    # test weight decay\n",
    "    param3 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_weight_decay = SGD([param3], weight_decay=0.1)\n",
    "    param3.grad = Tensor(np.array([3.0, 4.0]))\n",
    "    \n",
    "    optimizer_weight_decay.step()\n",
    "\n",
    "    \n",
    "    expected = param3.data - optimizer_weight_decay.learning_rate * (param3.grad.data + optimizer_weight_decay.weight_decay * param3.data)\n",
    "    assert np.allclose(expected, param3.data, rtol=0.05)\n",
    "    print('SGD Optimizer with weight decay works correctly')\n",
    "    \n",
    "    print(\"SGD optimizer works correctly!\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sgd_unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b014da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 23.954233333333338\n",
      "Iteration 10, loss 5.652310933047676\n",
      "Iteration 20, loss 0.7633906755512504\n",
      "Iteration 30, loss 1.008738277237916\n",
      "Iteration 40, loss 0.7229959892219938\n",
      "Iteration 50, loss 0.46474383421076865\n",
      "Iteration 60, loss 0.4137194474611481\n",
      "Iteration 70, loss 0.3795707359642195\n",
      "Iteration 80, loss 0.34471157095081856\n",
      "Iteration 90, loss 0.3204017834229631\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                    [4.0,5.0,8.0,10.0],\n",
    "                    [5.6,7.0, 11.1,1.0],\n",
    "                    [2.0, 3.0,0.0,-1.0],\n",
    "                    [4.0,5.0,-2.0, -10.0],\n",
    "                    [5.6,7.0, 11.9,12.0]]), requires_grad=True)\n",
    "y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0]), requires_grad=True)\n",
    "\n",
    "weight = Tensor(np.array([0.1, 0.2, 0.3,0.4]), requires_grad=True)\n",
    "bias = Tensor(np.array([0.0,0.0,0.0,0.0,0.0,0.0]), requires_grad=True)\n",
    "\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD([weight, bias],lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = x @ weight.transpose() + bias\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 ==0:\n",
    "        print(f'Iteration {i}, loss {loss.data}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55be76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 47.21571033682408\n",
      "Iteration 10, Loss: 25.476979983727873\n",
      "Iteration 20, Loss: 15.794613473628292\n",
      "Iteration 30, Loss: 11.10112486536548\n",
      "Iteration 40, Loss: 8.550502515815717\n",
      "Iteration 50, Loss: 6.980991399276939\n",
      "Iteration 60, Loss: 5.905896981729324\n",
      "Iteration 70, Loss: 5.111566859838638\n",
      "Iteration 80, Loss: 4.496759592264123\n",
      "Iteration 90, Loss: 4.008073169225105\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(linear.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9118b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: Adam Moments Updates ...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(v_hat, linear\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdama update moments update works correctly\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mUnit_test_adam_update_moments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m, in \u001b[0;36mUnit_test_adam_update_moments\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# for i, param in enumerate([linear.weight]):\u001b[39;00m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m m_hat, v_hat \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_moments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mm_buffers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mv_buffers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\babytorch\\minitorch\\optimizers\\optim.py:203\u001b[0m, in \u001b[0;36mAdam._update_moments\u001b[1;34m(self, i, gradient_data)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03mUpdate first and second moment estimates with bias correction.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mto counteract the zero-initialization bias in early training steps.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m#* initialize buffers if its the first time\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_buffers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_buffers[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(gradient_data)\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_buffers[i] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(gradient_data)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def Unit_test_adam_update_moments():\n",
    "    print('Unit Test: Adam Moments Updates ...')\n",
    "    linear = Linear(x.shape[1], y.size, bias=True)\n",
    "    optimizer = Adam([linear.weight])\n",
    "    \n",
    "    linear.weight.grad = np.random.random(linear.weight.shape)\n",
    "\n",
    "    # for i, param in enumerate([linear.weight]):\n",
    "    optimizer.step_count += 1\n",
    "    m_hat, v_hat = optimizer._update_moments(i, linear.weight.grad)\n",
    "    assert optimizer.m_buffers[0] is not None\n",
    "    assert optimizer.v_buffers[0] is not None\n",
    "    assert np.allclose(m_hat, linear.weight.grad)\n",
    "    assert np.allclose(v_hat, linear.weight.grad ** 2)\n",
    "    print('Adama update moments update works correctly')\n",
    "    \n",
    "Unit_test_adam_update_moments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cf8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: Adam Moments Updates ...\n",
      "Testing the first step\n",
      "Adam optimizer works for the first step\n",
      "Testing the second step\n",
      "Adam optimizer correctly on the second step\n",
      "Adam optimizer with adaptive learning rate works correctly.\n"
     ]
    }
   ],
   "source": [
    "def Unit_test_adam():\n",
    "    print('Unit Test: Adam Moments Updates ...')\n",
    "    linear = Linear(x.shape[1], y.size, bias=True)\n",
    "    optimizer = Adam([linear.weight])\n",
    "    print('Testing the first step')\n",
    "    \n",
    "    # set the gradient\n",
    "    linear.weight.grad = np.random.random(linear.weight.shape)\n",
    "    grad = linear.weight.grad.copy()\n",
    "    original_data = linear.weight.data.copy()\n",
    "    \n",
    "    # first step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # first moment: m = 0.9 * 0 + 0.1 * grad = 0.1 * grad\n",
    "    m = 0.1 * grad\n",
    "    \n",
    "    # second moment: v = 0.999 * 0 + 0.001 * grad ** 2 = 0.001 * grad ** 2\n",
    "    v = 0.001 * (grad ** 2)\n",
    "    \n",
    "    # bais correction\n",
    "    m_bias_correction = 1 - 0.9 ** optimizer.step_count\n",
    "    v_bias_correction = 1 - 0.999 * optimizer.step_count\n",
    "    \n",
    "    m_hat = m / m_bias_correction\n",
    "    v_hat = v / v_bias_correction\n",
    "    \n",
    "    #* paramter update\n",
    "    expected = original_data - optimizer.lr * m_hat / (np.sqrt(v_hat) + optimizer.eps)\n",
    "    assert np.allclose(expected, linear.weight.data)\n",
    "    assert optimizer.step_count == 1, f'Expected 1 in the first step, got {optimizer.step_count}'\n",
    "    print('Adam optimizer works for the first step')\n",
    "    \n",
    "    #* second step\n",
    "    print('Testing the second step')\n",
    "    \n",
    "    # set the gradient\n",
    "    optimizer = Adam([linear.bias])\n",
    "    linear.bias.grad = np.random.random(linear.bias.shape)\n",
    "    grad = linear.bias.grad.copy()\n",
    "    original_data = linear.bias.data.copy()\n",
    "    \n",
    "    # first step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # first moment: m = 0.9 * 0 + 0.1 * grad = 0.1 * grad\n",
    "    m = 0.1 * grad\n",
    "    \n",
    "    # second moment: v = 0.999 * 0 + 0.001 * grad ** 2 = 0.001 * grad ** 2\n",
    "    v = 0.001 * (grad ** 2)\n",
    "    \n",
    "    # bais correction\n",
    "    m_bias_correction = 1 - 0.9 ** optimizer.step_count\n",
    "    v_bias_correction = 1 - 0.999 * optimizer.step_count\n",
    "    \n",
    "    m_hat = m / m_bias_correction\n",
    "    v_hat = v / v_bias_correction\n",
    "    \n",
    "    #* paramter update\n",
    "    expected = original_data - optimizer.lr * m_hat / (np.sqrt(v_hat) + optimizer.eps)\n",
    "    assert np.allclose(expected, linear.bias.data), f'Excpected the parameter to be {expected}, got {linear.bias.data}'\n",
    "    # assert optimizer.step_count == 2, f'Expcted 2 in the second step, got {optimizer.step_count}'\n",
    "    print('Adam optimizer correctly on the second step')\n",
    "    \n",
    "    \n",
    "    print('Adam optimizer with adaptive learning rate works correctly.')\n",
    "    \n",
    "    \n",
    "Unit_test_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aed2cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 50.442521374401196\n",
      "Iteration 1000, Loss: 0.10882119762703754\n",
      "Iteration 2000, Loss: 0.08150233113642548\n",
      "Iteration 3000, Loss: 0.08066181607154065\n",
      "Iteration 4000, Loss: 0.08354036500605264\n",
      "Iteration 5000, Loss: 0.08114620766283201\n",
      "Iteration 6000, Loss: 0.08065318294872802\n",
      "Iteration 7000, Loss: 0.08095588088298407\n",
      "Iteration 8000, Loss: 0.0808139068152513\n",
      "Iteration 9000, Loss: 0.0935180902535618\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = Adam(linear.parameters(), lr=0.1,weight_decay=0.001)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(10000):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92bc5fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 69.4488174059303\n",
      "Iteration 10000, Loss: 0.0005150130165146818\n",
      "Iteration 20000, Loss: 0.0031929365709119344\n",
      "Iteration 30000, Loss: 0.00013526409449169465\n",
      "Iteration 40000, Loss: 0.00014145314344632132\n",
      "Iteration 50000, Loss: 0.00034390985544437743\n",
      "Iteration 60000, Loss: 0.0005161315429608722\n",
      "Iteration 70000, Loss: 0.00015714950489616934\n",
      "Iteration 80000, Loss: 0.00013737748793128595\n",
      "Iteration 90000, Loss: 0.0001668512829105135\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = AdamW(linear.parameters(), lr=0.1,weight_decay=0.001)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(100000):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10000 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e107f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[1.00490058 1.99482616 2.991096   2.99015039 4.11146404 4.98428141]\n",
       "  [1.00826543 1.99253823 2.98519634 2.98510882 4.17548962 4.97520121]\n",
       "  [1.01009757 1.9982266  2.99458421 2.99407155 4.17218911 4.99058666]\n",
       "  [1.00250596 2.00081124 3.00137516 3.00073049 4.0370774  5.00175835]\n",
       "  [0.99938528 2.00045078 3.00055027 3.00086048 3.98655643 5.0011929 ]\n",
       "  [1.02404827 2.01596511 3.01855065 3.01893041 4.28539767 5.03175339]], shape=(6, 6), grad_info= requires_grad=True),\n",
       " Tensor(data=[1. 2. 3. 3. 4. 5.], shape=(6,), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = linear(x)\n",
    "y_hat,y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9101419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of minitorch.optimizers.optim failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 274, in check\n",
      "    superreload(m, reload, self.old_objects, self.shell)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 330, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "AttributeError: 'types.GenericAlias' object has no attribute '__copy__'. Did you mean: '__bool__'?\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6ec8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.optimizers.optim import SGD\n",
    "from minitorch.nn.layers import Linear\n",
    "from minitorch.losses.losses import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "308650d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: SGD Optimizer ....\n",
      "[0.8 1.9] Tensor(data=[0.8 1.9])\n",
      "Basic SGD optimizer works correctly\n",
      "SGD Oprimizer with momentum works correctly \n",
      "SGD Optimizer with weight decay works correctly\n",
      "SGD optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def sgd_unit_test():\n",
    "    \"\"\"Test SGD optimizer implemetation\"\"\"\n",
    "    print('Unit Test: SGD Optimizer ....')\n",
    "    \n",
    "    #* basic optimizer test\n",
    "    param = Tensor(np.array([1.0,2.0], dtype=np.float32), requires_grad=True)\n",
    "    optimizer = SGD([param], lr=0.1)\n",
    "    param.grad = Tensor(np.array([2.0, 1.0], dtype=np.float32))\n",
    "    original_data = param.data.copy()\n",
    "    grad = param.grad.data\n",
    "    optimizer.step()\n",
    "    \n",
    "    expected = original_data - optimizer.learning_rate * grad\n",
    "    print(expected, param)\n",
    "    assert np.allclose(expected, param.data)\n",
    "    assert optimizer.step_count == 1\n",
    "    print('Basic SGD optimizer works correctly')\n",
    "    \n",
    "    # optimizer with momentum test\n",
    "    param2 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_momentum = SGD([param2], lr=0.1, momentum=0.9)\n",
    "    param2.grad = Tensor(np.array([2.0, 1.0]))\n",
    "    original_data = param2.data.copy()\n",
    "    grad = param2.grad.data\n",
    "    optimizer_momentum.step()\n",
    "    \n",
    "    expected = original_data - optimizer_momentum.learning_rate * grad\n",
    "    assert np.allclose(expected.data, param2.data)\n",
    "    assert optimizer_momentum.step_count == 1, f'step count expected to be 1 got {optimizer_momentum.step_count}'\n",
    "    print('SGD Oprimizer with momentum works correctly ')\n",
    "    \n",
    "    # test weight decay\n",
    "    param3 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_weight_decay = SGD([param3], weight_decay=0.1)\n",
    "    param3.grad = Tensor(np.array([3.0, 4.0]))\n",
    "    \n",
    "    optimizer_weight_decay.step()\n",
    "\n",
    "    \n",
    "    expected = param3.data - optimizer_weight_decay.learning_rate * (param3.grad.data + optimizer_weight_decay.weight_decay * param3.data)\n",
    "    assert np.allclose(expected, param3.data, rtol=0.05)\n",
    "    print('SGD Optimizer with weight decay works correctly')\n",
    "    \n",
    "    print(\"SGD optimizer works correctly!\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sgd_unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b014da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 23.954233333333338\n",
      "Iteration 10, loss 5.652311050735574\n",
      "Iteration 20, loss 0.7633906746555503\n",
      "Iteration 30, loss 1.0087382635539146\n",
      "Iteration 40, loss 0.722996016949936\n",
      "Iteration 50, loss 0.4647438389142266\n",
      "Iteration 60, loss 0.41371944880977113\n",
      "Iteration 70, loss 0.3795707374346713\n",
      "Iteration 80, loss 0.34471157328713914\n",
      "Iteration 90, loss 0.3204017848418598\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                    [4.0,5.0,8.0,10.0],\n",
    "                    [5.6,7.0, 11.1,1.0],\n",
    "                    [2.0, 3.0,0.0,-1.0],\n",
    "                    [4.0,5.0,-2.0, -10.0],\n",
    "                    [5.6,7.0, 11.9,12.0]]), requires_grad=True)\n",
    "y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0]), requires_grad=True)\n",
    "\n",
    "weight = Tensor(np.array([0.1, 0.2, 0.3,0.4]), requires_grad=True)\n",
    "bias = Tensor(np.array([0.0,0.0,0.0,0.0,0.0,0.0]), requires_grad=True)\n",
    "\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD([weight, bias],lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = x @ weight.transpose() + bias\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 ==0:\n",
    "        print(f'Iteration {i}, loss {loss.data}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d55be76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 32.32765511781126\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'memoryview'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Desktop\\babytorch\\minitorch\\optimizers\\optim.py:174\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m         grad_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum_buffers[i]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m#* update parameter: params = param - lr * grad_data\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrad_data\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m#* increament the counter\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'memoryview'"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(linear.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785742be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[0.97563864 1.94016067 2.89643249 2.88165812 3.87789192 4.84892   ]\n",
       "  [0.93302619 1.92472076 2.91890333 2.94093689 3.84201818 4.80422565]\n",
       "  [0.98119594 1.96314107 2.94134973 2.93649647 3.92432908 4.90634274]\n",
       "  [1.01765974 1.99893939 2.97366478 2.95317318 4.00001063 5.00016707]\n",
       "  [0.99789989 2.01005607 3.02579265 3.03520163 4.01977577 5.02441517]\n",
       "  [1.07140975 2.10925602 3.15262701 3.14948936 4.2261852  5.28008004]], shape=(6, 6), grad_info= requires_grad=True),\n",
       " Tensor(data=[1. 2. 3. 3. 4. 5.], shape=(6,), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = linear(x)\n",
    "y_hat, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

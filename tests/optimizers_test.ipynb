{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9101419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6ec8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.optimizers.optim import SGD, Adam\n",
    "from minitorch.nn.layers import Linear\n",
    "from minitorch.losses.losses import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "308650d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: SGD Optimizer ....\n",
      "[0.8 1.9] Tensor(data=[0.8 1.9])\n",
      "Basic SGD optimizer works correctly\n",
      "SGD Oprimizer with momentum works correctly \n",
      "SGD Optimizer with weight decay works correctly\n",
      "SGD optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def sgd_unit_test():\n",
    "    \"\"\"Test SGD optimizer implemetation\"\"\"\n",
    "    print('Unit Test: SGD Optimizer ....')\n",
    "    \n",
    "    #* basic optimizer test\n",
    "    param = Tensor(np.array([1.0,2.0], dtype=np.float32), requires_grad=True)\n",
    "    optimizer = SGD([param], lr=0.1)\n",
    "    param.grad = Tensor(np.array([2.0, 1.0], dtype=np.float32))\n",
    "    original_data = param.data.copy()\n",
    "    grad = param.grad.data\n",
    "    optimizer.step()\n",
    "    \n",
    "    expected = original_data - optimizer.learning_rate * grad\n",
    "    print(expected, param)\n",
    "    assert np.allclose(expected, param.data)\n",
    "    assert optimizer.step_count == 1\n",
    "    print('Basic SGD optimizer works correctly')\n",
    "    \n",
    "    # optimizer with momentum test\n",
    "    param2 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_momentum = SGD([param2], lr=0.1, momentum=0.9)\n",
    "    param2.grad = Tensor(np.array([2.0, 1.0]))\n",
    "    original_data = param2.data.copy()\n",
    "    grad = param2.grad.data\n",
    "    optimizer_momentum.step()\n",
    "    \n",
    "    expected = original_data - optimizer_momentum.learning_rate * grad\n",
    "    assert np.allclose(expected.data, param2.data)\n",
    "    assert optimizer_momentum.step_count == 1, f'step count expected to be 1 got {optimizer_momentum.step_count}'\n",
    "    print('SGD Oprimizer with momentum works correctly ')\n",
    "    \n",
    "    # test weight decay\n",
    "    param3 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_weight_decay = SGD([param3], weight_decay=0.1)\n",
    "    param3.grad = Tensor(np.array([3.0, 4.0]))\n",
    "    \n",
    "    optimizer_weight_decay.step()\n",
    "\n",
    "    \n",
    "    expected = param3.data - optimizer_weight_decay.learning_rate * (param3.grad.data + optimizer_weight_decay.weight_decay * param3.data)\n",
    "    assert np.allclose(expected, param3.data, rtol=0.05)\n",
    "    print('SGD Optimizer with weight decay works correctly')\n",
    "    \n",
    "    print(\"SGD optimizer works correctly!\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sgd_unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b014da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 23.954233333333338\n",
      "Iteration 10, loss 5.652310933047676\n",
      "Iteration 20, loss 0.7633906755512504\n",
      "Iteration 30, loss 1.008738277237916\n",
      "Iteration 40, loss 0.7229959892219938\n",
      "Iteration 50, loss 0.46474383421076865\n",
      "Iteration 60, loss 0.4137194474611481\n",
      "Iteration 70, loss 0.3795707359642195\n",
      "Iteration 80, loss 0.34471157095081856\n",
      "Iteration 90, loss 0.3204017834229631\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                    [4.0,5.0,8.0,10.0],\n",
    "                    [5.6,7.0, 11.1,1.0],\n",
    "                    [2.0, 3.0,0.0,-1.0],\n",
    "                    [4.0,5.0,-2.0, -10.0],\n",
    "                    [5.6,7.0, 11.9,12.0]]), requires_grad=True)\n",
    "y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0]), requires_grad=True)\n",
    "\n",
    "weight = Tensor(np.array([0.1, 0.2, 0.3,0.4]), requires_grad=True)\n",
    "bias = Tensor(np.array([0.0,0.0,0.0,0.0,0.0,0.0]), requires_grad=True)\n",
    "\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD([weight, bias],lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = x @ weight.transpose() + bias\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 ==0:\n",
    "        print(f'Iteration {i}, loss {loss.data}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d55be76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 87.98768337839843\n",
      "Iteration 10, Loss: 47.13314110006033\n",
      "Iteration 20, Loss: 28.82966828586299\n",
      "Iteration 30, Loss: 19.901986869317334\n",
      "Iteration 40, Loss: 15.023126775449306\n",
      "Iteration 50, Loss: 12.009344011760586\n",
      "Iteration 60, Loss: 9.94159845000162\n",
      "Iteration 70, Loss: 8.414411566179476\n",
      "Iteration 80, Loss: 7.2346019998700655\n",
      "Iteration 90, Loss: 6.299720787474553\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(linear.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9118b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: Adam Moments Updates ...\n",
      "Adama update moments update works correctly\n"
     ]
    }
   ],
   "source": [
    "def Unit_test_adam_update_moments():\n",
    "    print('Unit Test: Adam Moments Updates ...')\n",
    "    linear = Linear(x.shape[1], y.size, bias=True)\n",
    "    optimizer = Adam([linear.weight])\n",
    "    \n",
    "    linear.weight.grad = np.random.random(linear.weight.shape)\n",
    "\n",
    "    # for i, param in enumerate([linear.weight]):\n",
    "    optimizer.step_count += 1\n",
    "    m_hat, v_hat = optimizer._update_moments(i, linear.weight.grad)\n",
    "    assert optimizer.m_buffers[0] is not None\n",
    "    assert optimizer.v_buffers[0] is not None\n",
    "    assert np.allclose(m_hat, linear.weight.grad)\n",
    "    assert np.allclose(v_hat, linear.weight.grad ** 2)\n",
    "    print('Adama update moments update works correctly')\n",
    "    \n",
    "Unit_test_adam_update_moments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "45cf8bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: Adam Moments Updates ...\n",
      "Testing the first step\n",
      "Adam optimizer works for the first step\n",
      "Testing the second step\n",
      "Adam optimizer correctly on the second step\n",
      "Adam optimizer with adaptive learning rate works correctly.\n"
     ]
    }
   ],
   "source": [
    "def Unit_test_adam():\n",
    "    print('Unit Test: Adam Moments Updates ...')\n",
    "    linear = Linear(x.shape[1], y.size, bias=True)\n",
    "    optimizer = Adam([linear.weight])\n",
    "    print('Testing the first step')\n",
    "    \n",
    "    # set the gradient\n",
    "    linear.weight.grad = np.random.random(linear.weight.shape)\n",
    "    grad = linear.weight.grad.copy()\n",
    "    original_data = linear.weight.data.copy()\n",
    "    \n",
    "    # first step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # first moment: m = 0.9 * 0 + 0.1 * grad = 0.1 * grad\n",
    "    m = 0.1 * grad\n",
    "    \n",
    "    # second moment: v = 0.999 * 0 + 0.001 * grad ** 2 = 0.001 * grad ** 2\n",
    "    v = 0.001 * (grad ** 2)\n",
    "    \n",
    "    # bais correction\n",
    "    m_bias_correction = 1 - 0.9 ** optimizer.step_count\n",
    "    v_bias_correction = 1 - 0.999 * optimizer.step_count\n",
    "    \n",
    "    m_hat = m / m_bias_correction\n",
    "    v_hat = v / v_bias_correction\n",
    "    \n",
    "    #* paramter update\n",
    "    expected = original_data - optimizer.lr * m_hat / (np.sqrt(v_hat) + optimizer.eps)\n",
    "    assert np.allclose(expected, linear.weight.data)\n",
    "    assert optimizer.step_count == 1, f'Expected 1 in the first step, got {optimizer.step_count}'\n",
    "    print('Adam optimizer works for the first step')\n",
    "    \n",
    "    #* second step\n",
    "    print('Testing the second step')\n",
    "    \n",
    "    # set the gradient\n",
    "    optimizer = Adam([linear.bias])\n",
    "    linear.bias.grad = np.random.random(linear.bias.shape)\n",
    "    grad = linear.bias.grad.copy()\n",
    "    original_data = linear.bias.data.copy()\n",
    "    \n",
    "    # first step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # first moment: m = 0.9 * 0 + 0.1 * grad = 0.1 * grad\n",
    "    m = 0.1 * grad\n",
    "    \n",
    "    # second moment: v = 0.999 * 0 + 0.001 * grad ** 2 = 0.001 * grad ** 2\n",
    "    v = 0.001 * (grad ** 2)\n",
    "    \n",
    "    # bais correction\n",
    "    m_bias_correction = 1 - 0.9 ** optimizer.step_count\n",
    "    v_bias_correction = 1 - 0.999 * optimizer.step_count\n",
    "    \n",
    "    m_hat = m / m_bias_correction\n",
    "    v_hat = v / v_bias_correction\n",
    "    \n",
    "    #* paramter update\n",
    "    expected = original_data - optimizer.lr * m_hat / (np.sqrt(v_hat) + optimizer.eps)\n",
    "    assert np.allclose(expected, linear.bias.data), f'Excpected the parameter to be {expected}, got {linear.bias.data}'\n",
    "    # assert optimizer.step_count == 2, f'Expcted 2 in the second step, got {optimizer.step_count}'\n",
    "    print('Adam optimizer correctly on the second step')\n",
    "    \n",
    "    \n",
    "    print('Adam optimizer with adaptive learning rate works correctly.')\n",
    "    \n",
    "    \n",
    "Unit_test_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "aed2cd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 50.442521374401196\n",
      "Iteration 1000, Loss: 0.10882119762703754\n",
      "Iteration 2000, Loss: 0.08150233113642548\n",
      "Iteration 3000, Loss: 0.08066181607154065\n",
      "Iteration 4000, Loss: 0.08354036500605264\n",
      "Iteration 5000, Loss: 0.08114620766283201\n",
      "Iteration 6000, Loss: 0.08065318294872802\n",
      "Iteration 7000, Loss: 0.08095588088298407\n",
      "Iteration 8000, Loss: 0.0808139068152513\n",
      "Iteration 9000, Loss: 0.0935180902535618\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = Adam(linear.parameters(), lr=0.1,weight_decay=0.001)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(10000):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 1000 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

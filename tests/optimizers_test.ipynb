{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9101419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of minitorch.optimizers.optim failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 274, in check\n",
      "    superreload(m, reload, self.old_objects, self.shell)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"c:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 330, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "AttributeError: 'types.GenericAlias' object has no attribute '__copy__'. Did you mean: '__bool__'?\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a6ec8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.optimizers.optim import SGD\n",
    "from minitorch.nn.layers import Linear\n",
    "from minitorch.losses.losses import MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "308650d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Test: SGD Optimizer ....\n",
      "[0.8 1.9] Tensor(data=[0.8 1.9])\n",
      "Basic SGD optimizer works correctly\n",
      "SGD Oprimizer with momentum works correctly \n",
      "SGD Optimizer with weight decay works correctly\n",
      "SGD optimizer works correctly!\n"
     ]
    }
   ],
   "source": [
    "def sgd_unit_test():\n",
    "    \"\"\"Test SGD optimizer implemetation\"\"\"\n",
    "    print('Unit Test: SGD Optimizer ....')\n",
    "    \n",
    "    #* basic optimizer test\n",
    "    param = Tensor(np.array([1.0,2.0], dtype=np.float32), requires_grad=True)\n",
    "    optimizer = SGD([param], lr=0.1)\n",
    "    param.grad = Tensor(np.array([2.0, 1.0], dtype=np.float32))\n",
    "    original_data = param.data.copy()\n",
    "    grad = param.grad.data\n",
    "    optimizer.step()\n",
    "    \n",
    "    expected = original_data - optimizer.learning_rate * grad\n",
    "    print(expected, param)\n",
    "    assert np.allclose(expected, param.data)\n",
    "    assert optimizer.step_count == 1\n",
    "    print('Basic SGD optimizer works correctly')\n",
    "    \n",
    "    # optimizer with momentum test\n",
    "    param2 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_momentum = SGD([param2], lr=0.1, momentum=0.9)\n",
    "    param2.grad = Tensor(np.array([2.0, 1.0]))\n",
    "    original_data = param2.data.copy()\n",
    "    grad = param2.grad.data\n",
    "    optimizer_momentum.step()\n",
    "    \n",
    "    expected = original_data - optimizer_momentum.learning_rate * grad\n",
    "    assert np.allclose(expected.data, param2.data)\n",
    "    assert optimizer_momentum.step_count == 1, f'step count expected to be 1 got {optimizer_momentum.step_count}'\n",
    "    print('SGD Oprimizer with momentum works correctly ')\n",
    "    \n",
    "    # test weight decay\n",
    "    param3 = Tensor(np.array([1.0, 2.0]), requires_grad=True)\n",
    "    optimizer_weight_decay = SGD([param3], weight_decay=0.1)\n",
    "    param3.grad = Tensor(np.array([3.0, 4.0]))\n",
    "    \n",
    "    optimizer_weight_decay.step()\n",
    "\n",
    "    \n",
    "    expected = param3.data - optimizer_weight_decay.learning_rate * (param3.grad.data + optimizer_weight_decay.weight_decay * param3.data)\n",
    "    assert np.allclose(expected, param3.data, rtol=0.05)\n",
    "    print('SGD Optimizer with weight decay works correctly')\n",
    "    \n",
    "    print(\"SGD optimizer works correctly!\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sgd_unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b014da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss 23.954233333333338\n",
      "Iteration 10, loss 5.652310933047676\n",
      "Iteration 20, loss 0.7633906755512504\n",
      "Iteration 30, loss 1.008738277237916\n",
      "Iteration 40, loss 0.7229959892219938\n",
      "Iteration 50, loss 0.46474383421076865\n",
      "Iteration 60, loss 0.4137194474611481\n",
      "Iteration 70, loss 0.3795707359642195\n",
      "Iteration 80, loss 0.34471157095081856\n",
      "Iteration 90, loss 0.3204017834229631\n"
     ]
    }
   ],
   "source": [
    "x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                    [4.0,5.0,8.0,10.0],\n",
    "                    [5.6,7.0, 11.1,1.0],\n",
    "                    [2.0, 3.0,0.0,-1.0],\n",
    "                    [4.0,5.0,-2.0, -10.0],\n",
    "                    [5.6,7.0, 11.9,12.0]]), requires_grad=True)\n",
    "y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0]), requires_grad=True)\n",
    "\n",
    "weight = Tensor(np.array([0.1, 0.2, 0.3,0.4]), requires_grad=True)\n",
    "bias = Tensor(np.array([0.0,0.0,0.0,0.0,0.0,0.0]), requires_grad=True)\n",
    "\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD([weight, bias],lr=0.001, momentum=0.9, weight_decay=0.01)\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = x @ weight.transpose() + bias\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 ==0:\n",
    "        print(f'Iteration {i}, loss {loss.data}')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d55be76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 87.98768337839843\n",
      "Iteration 10, Loss: 47.13314110006033\n",
      "Iteration 20, Loss: 28.82966828586299\n",
      "Iteration 30, Loss: 19.901986869317334\n",
      "Iteration 40, Loss: 15.023126775449306\n",
      "Iteration 50, Loss: 12.009344011760586\n",
      "Iteration 60, Loss: 9.94159845000162\n",
      "Iteration 70, Loss: 8.414411566179476\n",
      "Iteration 80, Loss: 7.2346019998700655\n",
      "Iteration 90, Loss: 6.299720787474553\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(x.shape[1], y.size, bias=True)\n",
    "loss_fn = MSE()\n",
    "optimizer = SGD(linear.parameters(), lr=0.001, momentum=0.0, weight_decay=0.0)\n",
    "# y_hat = x @ weight + bias\n",
    "\n",
    "for i in range(100):\n",
    "    y_hat = linear(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.data}')\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "785742be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[ 0.99982871  1.45375876  1.47368904  1.61246175  2.19824459  2.04353783]\n",
       "  [ 0.98780671  2.7617519   2.36157125  2.72688993  3.39094325  3.65937865]\n",
       "  [-0.09930527 -0.0367062   2.43131109  2.80896348  2.901384    6.77009805]\n",
       "  [-1.16326663  1.37666317 -0.79306292  1.16934284 -0.34530165  1.15323477]\n",
       "  [-4.00648175  1.80063086 -2.73407138  1.56039148 -2.50270349  2.06285023]\n",
       "  [ 1.35207708  2.89950754  3.44403773  3.63446804  4.68520392  5.67660373]], shape=(6, 6), grad_info= requires_grad=True),\n",
       " Tensor(data=[1. 2. 3. 3. 4. 5.], shape=(6,), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = linear(x)\n",
    "y_hat, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc2b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05ce36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Tensor Creation...\n",
      "âœ… Tensor creation works correctly!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_unit_tensor_creation():\n",
    "    \"\"\"ðŸ§ª Test Tensor creation with various data types.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Tensor Creation...\")\n",
    "\n",
    "    # Test scalar creation\n",
    "    scalar = Tensor(np.array((5.0), dtype=np.float32))\n",
    "    assert scalar.data == 5.0\n",
    "    assert scalar.shape == ()\n",
    "    assert scalar.size == 1\n",
    "    assert not scalar.requires_grad\n",
    "    assert scalar.grad is None\n",
    "    assert scalar.dtype == np.float32\n",
    "\n",
    "    # # Test vector creation\n",
    "    vector = Tensor(np.array([1, 2, 3]))\n",
    "    assert np.array_equal(vector.data, np.array([1, 2, 3], dtype=np.float32))\n",
    "    assert vector.shape == (3,)\n",
    "    assert vector.size == 3\n",
    "\n",
    "    # Test matrix creation\n",
    "    matrix = Tensor(np.array([[1, 2], [3, 4]]))\n",
    "    assert np.array_equal(matrix.data, np.array([[1, 2], [3, 4]], dtype=np.float32))\n",
    "    assert matrix.shape == (2, 2)\n",
    "    assert matrix.size == 4\n",
    "\n",
    "    # Test gradient flag (dormant feature)\n",
    "    grad_tensor = Tensor(np.array([[1, 2]]), requires_grad=True)\n",
    "    assert grad_tensor.requires_grad\n",
    "    assert grad_tensor.grad is None  # Still None until Module 05\n",
    "\n",
    "    print(\"âœ… Tensor creation works correctly!\")\n",
    "\n",
    "test_unit_tensor_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5ac89d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Arithmetic Operations...\n",
      "[3 3 3]\n",
      "âœ… Arithmetic operations work correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_arithmetic_operations():\n",
    "    \"\"\"ðŸ§ª Test arithmetic operations with broadcasting.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Arithmetic Operations...\")\n",
    "\n",
    "    # Test tensor + tensor\n",
    "    a = Tensor(np.array([1, 2, 3]))\n",
    "    b = Tensor(np.array([4, 5, 6]))\n",
    "    result = a + b\n",
    "    assert np.array_equal(result.data, np.array([5, 7, 9], dtype=np.float32))\n",
    "\n",
    "    # Test tensor + scalar (very common in ML)\n",
    "    result = a + 10\n",
    "    assert np.array_equal(result.data, np.array([11, 12, 13], dtype=np.float32))\n",
    "    # Test broadcasting with different shapes (matrix + vector)\n",
    "    matrix = Tensor(np.array([[1, 2], [3, 4]]))\n",
    "    vector = Tensor(np.array([10, 20]))\n",
    "    result = matrix + vector\n",
    "    expected = np.array([[11, 22], [13, 24]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test subtraction (data centering)\n",
    "    result = b - a\n",
    "    print(result.data)\n",
    "    assert np.array_equal(result.data, np.array([3, 3, 3], dtype=np.float32))\n",
    "\n",
    "    # Test multiplication (scaling)\n",
    "    result = a * 2\n",
    "    assert np.array_equal(result.data, np.array([2, 4, 6], dtype=np.float32))\n",
    "\n",
    "    # Test division (normalization)\n",
    "    result = b / 2\n",
    "    assert np.array_equal(result.data, np.array([2.0, 2.5, 3.0], dtype=np.float32))\n",
    "\n",
    "    # Test chaining operations (common in ML pipelines)\n",
    "    normalized = (a - 2) / 2  # Center and scale\n",
    "    expected = np.array([-0.5, 0.0, 0.5], dtype=np.float32)\n",
    "    assert np.allclose(normalized.data, expected)\n",
    "\n",
    "    print(\"âœ… Arithmetic operations work correctly!\")\n",
    "\n",
    "test_unit_arithmetic_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7784bea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Matrix Multiplication...\n",
      "âœ… Matrix multiplication works correctly!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_unit_matrix_multiplication():\n",
    "    \"\"\"ðŸ§ª Test matrix multiplication operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Matrix Multiplication...\")\n",
    "\n",
    "    # Test 2Ã—2 matrix multiplication (basic case)\n",
    "    a = Tensor(np.array([[1, 2], [3, 4]]))  # 2Ã—2\n",
    "    b = Tensor(np.array([[5, 6], [7, 8]]))  # 2Ã—2\n",
    "    result = a.matmul(b)\n",
    "    # Expected: [[1Ã—5+2Ã—7, 1Ã—6+2Ã—8], [3Ã—5+4Ã—7, 3Ã—6+4Ã—8]] = [[19, 22], [43, 50]]\n",
    "    expected = np.array([[19, 22], [43, 50]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test rectangular matrices (common in neural networks)\n",
    "    c = Tensor(np.array([[1, 2, 3], [4, 5, 6]]))  # 2Ã—3 (like batch_size=2, features=3)\n",
    "    d = Tensor(np.array([[7, 8], [9, 10], [11, 12]]))  # 3Ã—2 (like features=3, outputs=2)\n",
    "    result = c.matmul(d)\n",
    "    # Expected: [[1Ã—7+2Ã—9+3Ã—11, 1Ã—8+2Ã—10+3Ã—12], [4Ã—7+5Ã—9+6Ã—11, 4Ã—8+5Ã—10+6Ã—12]]\n",
    "    expected = np.array([[58, 64], [139, 154]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test matrix-vector multiplication (common in forward pass)\n",
    "    matrix = Tensor(np.array([[1, 2, 3], [4, 5, 6]]))  # 2Ã—3\n",
    "    vector = Tensor(np.array([[1], [2], [3]]))  # 3Ã—1 (conceptually)\n",
    "    result = matrix.matmul(vector)\n",
    "    # Expected: [1Ã—1+2Ã—2+3Ã—3, 4Ã—1+5Ã—2+6Ã—3] = [14, 32]\n",
    "    expected = np.array([[14], [32]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Test shape validation - should raise clear error\n",
    "    try:\n",
    "        incompatible_a = Tensor(np.array([[1, 2]]))     # 1Ã—2\n",
    "        incompatible_b = Tensor(np.array([[1], [2], [3]]))  # 3Ã—1\n",
    "        incompatible_a.matmul(incompatible_b)  # 1Ã—2 @ 3Ã—1 should fail (2 â‰  3)\n",
    "        assert False, \"Should have raised ValueError for incompatible shapes\"\n",
    "    except ValueError as e:\n",
    "        assert \"Inner dimensions must match\" in str(e)\n",
    "        assert \"2 â‰  3\" in str(e)  # Should show specific dimensions\n",
    "\n",
    "    print(\"âœ… Matrix multiplication works correctly!\")\n",
    "    \n",
    "test_unit_matrix_multiplication()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00959c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Shape Manipulation...\n",
      "âœ… Shape manipulation works correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_shape_manipulation():\n",
    "    \"\"\"ðŸ§ª Test reshape and transpose operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Shape Manipulation...\")\n",
    "\n",
    "    # Test basic reshape (flatten â†’ matrix)\n",
    "    tensor = Tensor(np.array([[1, 2, 3, 4, 5, 6]]))  # Shape: (6,)\n",
    "    reshaped = tensor.reshape(2, 3)      # Shape: (2, 3)\n",
    "    assert reshaped.shape == (2, 3)\n",
    "    expected = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n",
    "    assert np.array_equal(reshaped.data, expected)\n",
    "\n",
    "    # Test reshape with tuple (alternative calling style)\n",
    "    reshaped2 = tensor.reshape((3, 2))   # Shape: (3, 2)\n",
    "    assert reshaped2.shape == (3, 2)\n",
    "    expected2 = np.array([[1, 2], [3, 4], [5, 6]], dtype=np.float32)\n",
    "    assert np.array_equal(reshaped2.data, expected2)\n",
    "\n",
    "    # Test reshape with -1 (automatic dimension inference)\n",
    "    auto_reshaped = tensor.reshape(2, -1)  # Should infer -1 as 3\n",
    "    assert auto_reshaped.shape == (2, 3)\n",
    "\n",
    "    # Test reshape validation - should raise error for incompatible sizes\n",
    "    try:\n",
    "        tensor.reshape(2, 2)  # 6 elements can't fit in 2Ã—2=4\n",
    "        assert False, \"Should have raised ValueError\"\n",
    "    except ValueError as e:\n",
    "        assert \"Total elements must match\" in str(e)\n",
    "        assert \"6 â‰  4\" in str(e)\n",
    "\n",
    "    # Test matrix transpose (most common case)\n",
    "    matrix = Tensor(np.array([[1, 2, 3], [4, 5, 6]]))  # (2, 3)\n",
    "    transposed = matrix.transpose()          # (3, 2)\n",
    "    assert transposed.shape == (3, 2)\n",
    "    expected = np.array([[1, 4], [2, 5], [3, 6]], dtype=np.float32)\n",
    "    assert np.array_equal(transposed.data, expected)\n",
    "\n",
    "    # Test 1D transpose (should be identity)\n",
    "    vector = Tensor(np.array([1, 2, 3]))  # Shape: (3,)\n",
    "    vector_t = vector.transpose()\n",
    "    assert np.array_equal(vector.data, vector_t.data)\n",
    "\n",
    "    # Test specific dimension transpose\n",
    "    tensor_3d = Tensor(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))  # (2, 2, 2)\n",
    "    swapped = tensor_3d.transpose(0, 2)  # Swap first and last dimensions\n",
    "    assert swapped.shape == (2, 2, 2)  # Same shape but data rearranged\n",
    "\n",
    "    # Test neural network reshape pattern (flatten for MLP)\n",
    "    batch_images = Tensor(np.random.rand(2, 3, 4))  # (batch=2, height=3, width=4)\n",
    "    flattened = batch_images.reshape(2, -1)  # (batch=2, features=12)\n",
    "    assert flattened.shape == (2, 12)\n",
    "\n",
    "    print(\"âœ… Shape manipulation works correctly!\")\n",
    "\n",
    "test_unit_shape_manipulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68f8bc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Unit Test: Reduction Operations...\n",
      "âœ… Reduction operations work correctly!\n"
     ]
    }
   ],
   "source": [
    "def test_unit_reduction_operations():\n",
    "    \"\"\"ðŸ§ª Test reduction operations.\"\"\"\n",
    "    print(\"ðŸ§ª Unit Test: Reduction Operations...\")\n",
    "\n",
    "    matrix = Tensor(np.array([[1, 2, 3], [4, 5, 6]]))  # Shape: (2, 3)\n",
    "\n",
    "    # Test sum all elements (common for loss computation)\n",
    "    total = matrix.sum()\n",
    "    assert total.data == 21.0  # 1+2+3+4+5+6\n",
    "    assert total.shape == ()   # Scalar result\n",
    "\n",
    "    # Test sum along axis 0 (columns) - batch dimension reduction\n",
    "    col_sum = matrix.sum(axis=0)\n",
    "    expected_col = np.array([5, 7, 9], dtype=np.float32)  # [1+4, 2+5, 3+6]\n",
    "    assert np.array_equal(col_sum.data, expected_col)\n",
    "    assert col_sum.shape == (3,)\n",
    "\n",
    "    # Test sum along axis 1 (rows) - feature dimension reduction\n",
    "    row_sum = matrix.sum(axis=1)\n",
    "    expected_row = np.array([6, 15], dtype=np.float32)  # [1+2+3, 4+5+6]\n",
    "    assert np.array_equal(row_sum.data, expected_row)\n",
    "    assert row_sum.shape == (2,)\n",
    "\n",
    "    # Test mean (average loss computation)\n",
    "    avg = matrix.mean()\n",
    "    assert np.isclose(avg.data, 3.5)  # 21/6\n",
    "    assert avg.shape == ()\n",
    "\n",
    "    # Test mean along axis (batch normalization pattern)\n",
    "    col_mean = matrix.mean(axis=0)\n",
    "    expected_mean = np.array([2.5, 3.5, 4.5], dtype=np.float32)  # [5/2, 7/2, 9/2]\n",
    "    assert np.allclose(col_mean.data, expected_mean)\n",
    "\n",
    "    # Test max (finding best predictions)\n",
    "    maximum = matrix.max()\n",
    "    assert maximum.data == 6.0\n",
    "    assert maximum.shape == ()\n",
    "\n",
    "    # Test max along axis (argmax-like operation)\n",
    "    row_max = matrix.max(axis=1)\n",
    "    expected_max = np.array([3, 6], dtype=np.float32)  # [max(1,2,3), max(4,5,6)]\n",
    "    assert np.array_equal(row_max.data, expected_max)\n",
    "\n",
    "    # Test keepdims (important for broadcasting)\n",
    "    sum_keepdims = matrix.sum(axis=1, keepdims=True)\n",
    "    assert sum_keepdims.shape == (2, 1)  # Maintains 2D shape\n",
    "    expected_keepdims = np.array([[6], [15]], dtype=np.float32)\n",
    "    assert np.array_equal(sum_keepdims.data, expected_keepdims)\n",
    "\n",
    "    # Test 3D reduction (simulating global average pooling)\n",
    "    tensor_3d = Tensor(np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]))  # (2, 2, 2)\n",
    "    spatial_mean = tensor_3d.mean(axis=(1, 2))  # Average across spatial dimensions\n",
    "    assert spatial_mean.shape == (2,)  # One value per batch item\n",
    "\n",
    "    print(\"âœ… Reduction operations work correctly!\")\n",
    "\n",
    "test_unit_reduction_operations()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477ef6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analyzing Memory Access Patterns...\n",
      "============================================================\n",
      "\n",
      "Testing with 2000Ã—2000 matrix (15625.0 MB)\n",
      "------------------------------------------------------------\n",
      "\n",
      "ðŸ”¬ Test 1: Row-wise Access (Cache-Friendly)\n",
      "   Time: 30.9ms\n",
      "   Access pattern: Sequential (follows memory layout)\n",
      "\n",
      "ðŸ”¬ Test 2: Column-wise Access (Cache-Unfriendly)\n",
      "   Time: 65.8ms\n",
      "   Access pattern: Strided (jumps 8000 bytes per element)\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š PERFORMANCE IMPACT:\n",
      "   Slowdown factor: 2.13Ã— (2.1Ã— slower)\n",
      "   Cache misses cause 113% performance loss\n",
      "\n",
      "ðŸ’¡ KEY INSIGHTS:\n",
      "   1. Memory layout matters: Row-major (C-style) storage is sequential\n",
      "   2. Cache lines are ~64 bytes: Row access loads nearby elements \"for free\"\n",
      "   3. Column access misses cache: Must reload from DRAM every time\n",
      "   4. This is O(n) algorithm but 2.1Ã— different wall-clock time!\n",
      "\n",
      "ðŸš€ REAL-WORLD IMPLICATIONS:\n",
      "   â€¢ CNNs use NCHW format (channels sequential) for cache efficiency\n",
      "   â€¢ Matrix multiplication optimized with blocking (tile into cache-sized chunks)\n",
      "   â€¢ Transpose is expensive (2.1Ã—) because it changes memory layout\n",
      "   â€¢ This is why GPU frameworks obsess over memory coalescing\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "BYTES_PER_FLOAT32 = 4\n",
    "MB_TO_BYTES = 1024\n",
    "\n",
    "def analyze_memory_layout():\n",
    "    \"\"\"ðŸ“Š Demonstrate cache effects with row vs column access patterns.\"\"\"\n",
    "    print(\"ðŸ“Š Analyzing Memory Access Patterns...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create a moderately-sized matrix (large enough to show cache effects)\n",
    "    size = 2000\n",
    "    matrix = Tensor(np.random.rand(size, size))\n",
    "\n",
    "    import time\n",
    "\n",
    "    print(f\"\\nTesting with {size}Ã—{size} matrix ({matrix.size * BYTES_PER_FLOAT32 / MB_TO_BYTES:.1f} MB)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Test 1: Row-wise access (cache-friendly)\n",
    "    # Memory layout: [row0][row1][row2]... stored contiguously\n",
    "    print(\"\\nðŸ”¬ Test 1: Row-wise Access (Cache-Friendly)\")\n",
    "    start = time.time()\n",
    "    row_sums = []\n",
    "    for i in range(size):\n",
    "        row_sum = matrix.data[i, :].sum()  # Access entire row sequentially\n",
    "        row_sums.append(row_sum)\n",
    "    row_time = time.time() - start\n",
    "    print(f\"   Time: {row_time*1000:.1f}ms\")\n",
    "    print(\"   Access pattern: Sequential (follows memory layout)\")\n",
    "\n",
    "    # Test 2: Column-wise access (cache-unfriendly)\n",
    "    # Must jump between rows, poor spatial locality\n",
    "    print(\"\\nðŸ”¬ Test 2: Column-wise Access (Cache-Unfriendly)\")\n",
    "    start = time.time()\n",
    "    col_sums = []\n",
    "    for j in range(size):\n",
    "        col_sum = matrix.data[:, j].sum()  # Access entire column with large strides\n",
    "        col_sums.append(col_sum)\n",
    "    col_time = time.time() - start\n",
    "    print(f\"   Time: {col_time*1000:.1f}ms\")\n",
    "    print(f\"   Access pattern: Strided (jumps {size * BYTES_PER_FLOAT32} bytes per element)\")\n",
    "\n",
    "    # Calculate slowdown\n",
    "    slowdown = col_time / row_time\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸ“Š PERFORMANCE IMPACT:\")\n",
    "    print(f\"   Slowdown factor: {slowdown:.2f}Ã— ({col_time/row_time:.1f}Ã— slower)\")\n",
    "    print(f\"   Cache misses cause {(slowdown-1)*100:.0f}% performance loss\")\n",
    "\n",
    "    # Educational insights\n",
    "    print(\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
    "    print(\"   1. Memory layout matters: Row-major (C-style) storage is sequential\")\n",
    "    print(\"   2. Cache lines are ~64 bytes: Row access loads nearby elements \\\"for free\\\"\")\n",
    "    print(\"   3. Column access misses cache: Must reload from DRAM every time\")\n",
    "    print(f\"   4. This is O(n) algorithm but {slowdown:.1f}Ã— different wall-clock time!\")\n",
    "\n",
    "    print(\"\\nðŸš€ REAL-WORLD IMPLICATIONS:\")\n",
    "    print(\"   â€¢ CNNs use NCHW format (channels sequential) for cache efficiency\")\n",
    "    print(\"   â€¢ Matrix multiplication optimized with blocking (tile into cache-sized chunks)\")\n",
    "    print(f\"   â€¢ Transpose is expensive ({slowdown:.1f}Ã—) because it changes memory layout\")\n",
    "    print(\"   â€¢ This is why GPU frameworks obsess over memory coalescing\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "analyze_memory_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c26c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª RUNNING MODULE INTEGRATION TEST\n",
      "==================================================\n",
      "Running unit tests...\n",
      "ðŸ§ª Unit Test: Tensor Creation...\n",
      "âœ… Tensor creation works correctly!\n",
      "ðŸ§ª Unit Test: Arithmetic Operations...\n",
      "[3 3 3]\n",
      "âœ… Arithmetic operations work correctly!\n",
      "ðŸ§ª Unit Test: Matrix Multiplication...\n",
      "âœ… Matrix multiplication works correctly!\n",
      "ðŸ§ª Unit Test: Shape Manipulation...\n",
      "âœ… Shape manipulation works correctly!\n",
      "ðŸ§ª Unit Test: Reduction Operations...\n",
      "âœ… Reduction operations work correctly!\n",
      "\n",
      "Running integration scenarios...\n",
      "ðŸ§ª Integration Test: Two-Layer Neural Network...\n",
      "âœ… Two-layer neural network computation works!\n",
      "ðŸ§ª Integration Test: Gradient System Readiness...\n",
      "âœ… Gradient system ready for Module 05!\n",
      "ðŸ§ª Integration Test: Complex Shape Operations...\n",
      "Tensor(data=[[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[ 7  8  9]\n",
      "  [10 11 12]]])\n",
      "Tensor(data=[3.5 9.5])\n",
      "Tensor(data=[[ 1  2  3  4  5  6]\n",
      " [ 7  8  9 10 11 12]])\n",
      "Tensor(data=[[[ 1  4]\n",
      "  [ 2  5]\n",
      "  [ 3  6]]\n",
      "\n",
      " [[ 7 10]\n",
      "  [ 8 11]\n",
      "  [ 9 12]]])\n",
      "âœ… Complex shape operations work!\n",
      "ðŸ§ª Integration Test: Broadcasting Edge Cases...\n",
      "âœ… Broadcasting edge cases work!\n",
      "\n",
      "==================================================\n",
      "ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\n",
      "Run: tito module complete 01_tensor\n"
     ]
    }
   ],
   "source": [
    "def test_module():\n",
    "    \"\"\"ðŸ§ª Module Test: Complete Integration\n",
    "\n",
    "    Comprehensive test of entire module functionality.\n",
    "\n",
    "    This final test runs before module summary to ensure:\n",
    "    - All unit tests pass\n",
    "    - Functions work together correctly\n",
    "    - Module is ready for integration with TinyTorch\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª RUNNING MODULE INTEGRATION TEST\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Run all unit tests\n",
    "    print(\"Running unit tests...\")\n",
    "    test_unit_tensor_creation()\n",
    "    test_unit_arithmetic_operations()\n",
    "    test_unit_matrix_multiplication()\n",
    "    test_unit_shape_manipulation()\n",
    "    test_unit_reduction_operations()\n",
    "\n",
    "    print(\"\\nRunning integration scenarios...\")\n",
    "\n",
    "    # Test realistic neural network computation\n",
    "    print(\"ðŸ§ª Integration Test: Two-Layer Neural Network...\")\n",
    "\n",
    "    # Create input data (2 samples, 3 features)\n",
    "    x = Tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "\n",
    "    # First layer: 3 inputs â†’ 4 hidden units\n",
    "    W1 = Tensor(np.array([[0.1, 0.2, 0.3, 0.4],\n",
    "                [0.5, 0.6, 0.7, 0.8],\n",
    "                [0.9, 1.0, 1.1, 1.2]]))\n",
    "    b1 = Tensor(np.array([0.1, 0.2, 0.3, 0.4]))\n",
    "\n",
    "    # Forward pass: hidden = xW1 + b1\n",
    "    hidden = x.matmul(W1) + b1\n",
    "    assert hidden.shape == (2, 4), f\"Expected (2, 4), got {hidden.shape}\"\n",
    "\n",
    "    # Second layer: 4 hidden â†’ 2 outputs\n",
    "    W2 = Tensor(np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6], [0.7, 0.8]]))\n",
    "    b2 = Tensor(np.array([0.1, 0.2]))\n",
    "\n",
    "    # Output layer: output = hiddenW2 + b2\n",
    "    output = hidden.matmul(W2) + b2\n",
    "    assert output.shape == (2, 2), f\"Expected (2, 2), got {output.shape}\"\n",
    "\n",
    "    # Verify data flows correctly (no NaN, reasonable values)\n",
    "    assert not np.isnan(output.data).any(), \"Output contains NaN values\"\n",
    "    assert np.isfinite(output.data).all(), \"Output contains infinite values\"\n",
    "\n",
    "    print(\"âœ… Two-layer neural network computation works!\")\n",
    "\n",
    "    # Test gradient attributes are preserved and functional\n",
    "    print(\"ðŸ§ª Integration Test: Gradient System Readiness...\")\n",
    "    grad_tensor = Tensor(np.array([1, 2, 3]), requires_grad=True)\n",
    "    result = grad_tensor + 5\n",
    "    assert grad_tensor.requires_grad, \"requires_grad not preserved\"\n",
    "    assert grad_tensor.grad is None, \"grad should still be None\"\n",
    "\n",
    "    # Test backward() doesn't crash (even though it does nothing)\n",
    "    grad_tensor.backward()  # Should not raise any exception\n",
    "\n",
    "    print(\"âœ… Gradient system ready for Module 05!\")\n",
    "\n",
    "    # Test complex shape manipulations\n",
    "    print(\"ðŸ§ª Integration Test: Complex Shape Operations...\")\n",
    "    data = Tensor(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]))\n",
    "\n",
    "    # Reshape to 3D tensor (simulating batch processing)\n",
    "    tensor_3d = data.reshape(2, 2, 3)  # (batch=2, height=2, width=3)\n",
    "    print(tensor_3d)\n",
    "    assert tensor_3d.shape == (2, 2, 3)\n",
    "\n",
    "    # Global average pooling simulation\n",
    "    pooled = tensor_3d.mean(axis=(1, 2))  # Average across spatial dimensions\n",
    "    print(pooled)\n",
    "    assert pooled.shape == (2,), f\"Expected (2,), got {pooled.shape}\"\n",
    "\n",
    "    # Flatten for MLP\n",
    "    flattened = tensor_3d.reshape(2, -1)  # (batch, features)\n",
    "    print(flattened)\n",
    "    assert flattened.shape == (2, 6)\n",
    "\n",
    "    # Transpose for different operations\n",
    "    transposed = tensor_3d.transpose()  # Should transpose last two dims\n",
    "    print(transposed)\n",
    "    assert transposed.shape == (2, 3, 2)\n",
    "\n",
    "    print(\"âœ… Complex shape operations work!\")\n",
    "\n",
    "    # Test broadcasting edge cases\n",
    "    print(\"ðŸ§ª Integration Test: Broadcasting Edge Cases...\")\n",
    "\n",
    "    # Scalar broadcasting\n",
    "    scalar = Tensor(np.array([5.0]))\n",
    "    vector = Tensor(np.array([1, 2, 3]))\n",
    "    result = scalar + vector  # Should broadcast scalar to vector shape\n",
    "    expected = np.array([6, 7, 8], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    # Matrix + vector broadcasting\n",
    "    matrix = Tensor(np.array([[1, 2], [3, 4]]))\n",
    "    vec = Tensor(np.array([10, 20]))\n",
    "    result = matrix + vec\n",
    "    expected = np.array([[11, 22], [13, 24]], dtype=np.float32)\n",
    "    assert np.array_equal(result.data, expected)\n",
    "\n",
    "    print(\"âœ… Broadcasting edge cases work!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸŽ‰ ALL TESTS PASSED! Module ready for export.\")\n",
    "    print(\"Run: tito module complete 01_tensor\")\n",
    "    \n",
    "test_module()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

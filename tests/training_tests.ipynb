{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162d12bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#* get autorelod for the notesbook\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6111b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.train.training import CosineSchedule, clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9ff8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Tests: Testing the cosine scheduler ....\n",
      "Learning rate at epoch 0: 0.1000\n",
      "Learning rate at epoch 25: 0.0550\n",
      "Learning rate at epoch 50: 0.0550\n",
      "Learning rate at epoch 100: 0.0100\n",
      "Cosine Scheduler works perfectly\n"
     ]
    }
   ],
   "source": [
    "def unit_tests_cosine_scheduler():\n",
    "    print('Unit Tests: Testing the cosine scheduler ....')\n",
    "    scheduler = CosineSchedule()\n",
    "    tolerance = 1e-6\n",
    "    \n",
    "    #* Test basic schedule\n",
    "    lr_start = scheduler.get_lr(0)\n",
    "    lr_quarter = scheduler.get_lr(25)\n",
    "    lr_middle = scheduler.get_lr(50)\n",
    "    lr_end = scheduler.get_lr(100)\n",
    "    \n",
    "    print(f\"Learning rate at epoch 0: {lr_start:.4f}\")\n",
    "    print(f\"Learning rate at epoch 25: {lr_middle:.4f}\")\n",
    "    print(f\"Learning rate at epoch 50: {lr_middle:.4f}\")\n",
    "    print(f\"Learning rate at epoch 100: {lr_end:.4f}\")\n",
    "    \n",
    "    #* validate behavior\n",
    "    assert abs(lr_start - 0.1) < tolerance, f'Expected 0.1 at start, got {lr_start}'\n",
    "    assert abs(lr_end - 0.01) < tolerance, f'Expected 0.01 at the end, got {lr_end}'\n",
    "    assert 0.01 < lr_middle < 0.1, f'Expected middle lr to be between 0.01 and 0.1, got {lr_middle}'\n",
    "    \n",
    "    #* monotonic test\n",
    "    assert lr_quarter > lr_middle, 'Lr should decrease monotonically in first half'\n",
    "    \n",
    "    print('Cosine Scheduler works perfectly')\n",
    "    \n",
    "unit_tests_cosine_scheduler()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f36a7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Tests: Testing the clip_grad_norm function ....\n",
      "clip_grad_norm works perfectly\n"
     ]
    }
   ],
   "source": [
    "def unit_tests_clip_grad_norm():\n",
    "    print('Unit Tests: Testing the clip_grad_norm function ....')\n",
    "    x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                        [4.0,5.0,8.0,10.0],\n",
    "                        [5.6,7.0, 11.1,1.0],\n",
    "                        [2.0, 3.0,0.0,-1.0],\n",
    "                        [4.0,5.0,-2.0, -10.0],\n",
    "                        [5.6,7.0, 11.9,12.0]], dtype='float32'), requires_grad=True)\n",
    "    y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0], dtype='float32'), requires_grad=True)\n",
    "    \n",
    "    \n",
    "    x.grad = np.random.randint(x.shape[0], size=x.shape, dtype='int32').astype('float32')\n",
    "    y.grad = np.random.randint(y.shape[0], size=y.shape, dtype='int32').astype('float32')\n",
    "    \n",
    "    #* clip manually to verify\n",
    "    x_grad = np.sum(x.grad ** 2)\n",
    "    y_grad = np.sum(y.grad ** 2)\n",
    "    total_norm = np.sqrt(x_grad + y_grad)\n",
    "    \n",
    "    if total_norm > 1.0:\n",
    "        clip_coef = 1.0 / (total_norm + 1e-6)\n",
    "        x.grad *= clip_coef\n",
    "        y.grad *= clip_coef\n",
    "        \n",
    "    #* now use the function to verify it does the same thing\n",
    "    x_copy = x.copy()\n",
    "    y_copy = y.copy()\n",
    "    x_copy.grad = x.grad.copy()\n",
    "    y_copy.grad = y.grad.copy()\n",
    "    \n",
    "    norm = clip_grad_norm([x_copy,y_copy], max_norm=1.0)\n",
    "    \n",
    "    \n",
    "    #* compare manual clipping with function clipping\n",
    "    assert np.allclose(x_copy.grad, x.grad), \"x grad should be the same\"\n",
    "    assert np.allclose(y_copy.grad, y.grad), \"y grad should be the same\"\n",
    "    # assert norm > 1.0, f\"Total norm should be greater than 1.0 before clipping, got {norm:.4f}\"\n",
    "    print('clip_grad_norm works perfectly')\n",
    "    \n",
    "    \n",
    "unit_tests_clip_grad_norm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "162d12bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#* get autorelod for the notesbook\n",
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6111b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.train.training import CosineSchedule, clip_grad_norm,Trainer\n",
    "from minitorch.nn.layers import Linear\n",
    "from minitorch.optimizers.optim import SGD, Adam, AdamW\n",
    "from minitorch.losses.losses import MSE\n",
    "from minitorch.dataloaders.dataloader import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9ff8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Tests: Testing the cosine scheduler ....\n",
      "Learning rate at epoch 0: 0.1000\n",
      "Learning rate at epoch 25: 0.0550\n",
      "Learning rate at epoch 50: 0.0550\n",
      "Learning rate at epoch 100: 0.0100\n",
      "Cosine Scheduler works perfectly\n"
     ]
    }
   ],
   "source": [
    "def unit_tests_cosine_scheduler():\n",
    "    print('Unit Tests: Testing the cosine scheduler ....')\n",
    "    scheduler = CosineSchedule()\n",
    "    tolerance = 1e-6\n",
    "    \n",
    "    #* Test basic schedule\n",
    "    lr_start = scheduler.get_lr(0)\n",
    "    lr_quarter = scheduler.get_lr(25)\n",
    "    lr_middle = scheduler.get_lr(50)\n",
    "    lr_end = scheduler.get_lr(100)\n",
    "    \n",
    "    print(f\"Learning rate at epoch 0: {lr_start:.4f}\")\n",
    "    print(f\"Learning rate at epoch 25: {lr_middle:.4f}\")\n",
    "    print(f\"Learning rate at epoch 50: {lr_middle:.4f}\")\n",
    "    print(f\"Learning rate at epoch 100: {lr_end:.4f}\")\n",
    "    \n",
    "    #* validate behavior\n",
    "    assert abs(lr_start - 0.1) < tolerance, f'Expected 0.1 at start, got {lr_start}'\n",
    "    assert abs(lr_end - 0.01) < tolerance, f'Expected 0.01 at the end, got {lr_end}'\n",
    "    assert 0.01 < lr_middle < 0.1, f'Expected middle lr to be between 0.01 and 0.1, got {lr_middle}'\n",
    "    \n",
    "    #* monotonic test\n",
    "    assert lr_quarter > lr_middle, 'Lr should decrease monotonically in first half'\n",
    "    \n",
    "    print('Cosine Scheduler works perfectly')\n",
    "    \n",
    "unit_tests_cosine_scheduler()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f36a7dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit Tests: Testing the clip_grad_norm function ....\n",
      "clip_grad_norm works perfectly\n"
     ]
    }
   ],
   "source": [
    "def unit_tests_clip_grad_norm():\n",
    "    print('Unit Tests: Testing the clip_grad_norm function ....')\n",
    "    x = Tensor(np.array([[2.0, 3.0, 4.6,7.0],\n",
    "                        [4.0,5.0,8.0,10.0],\n",
    "                        [5.6,7.0, 11.1,1.0],\n",
    "                        [2.0, 3.0,0.0,-1.0],\n",
    "                        [4.0,5.0,-2.0, -10.0],\n",
    "                        [5.6,7.0, 11.9,12.0]], dtype='float32'), requires_grad=True)\n",
    "    y = Tensor(np.array([1.0, 2.0, 3.0, 3.0, 4.0,5.0], dtype='float32'), requires_grad=True)\n",
    "    \n",
    "    \n",
    "    x.grad = np.random.randint(x.shape[0], size=x.shape, dtype='int32').astype('float32')\n",
    "    y.grad = np.random.randint(y.shape[0], size=y.shape, dtype='int32').astype('float32')\n",
    "    \n",
    "    #* clip manually to verify\n",
    "    x_grad = np.sum(x.grad ** 2)\n",
    "    y_grad = np.sum(y.grad ** 2)\n",
    "    total_norm = np.sqrt(x_grad + y_grad)\n",
    "    \n",
    "    if total_norm > 1.0:\n",
    "        clip_coef = 1.0 / (total_norm + 1e-6)\n",
    "        x.grad *= clip_coef\n",
    "        y.grad *= clip_coef\n",
    "        \n",
    "    #* now use the function to verify it does the same thing\n",
    "    x_copy = x.copy()\n",
    "    y_copy = y.copy()\n",
    "    x_copy.grad = x.grad.copy()\n",
    "    y_copy.grad = y.grad.copy()\n",
    "    \n",
    "    norm = clip_grad_norm([x_copy,y_copy], max_norm=1.0)\n",
    "    \n",
    "    \n",
    "    #* compare manual clipping with function clipping\n",
    "    assert np.allclose(x_copy.grad, x.grad), \"x grad should be the same\"\n",
    "    assert np.allclose(y_copy.grad, y.grad), \"y grad should be the same\"\n",
    "    # assert norm > 1.0, f\"Total norm should be greater than 1.0 before clipping, got {norm:.4f}\"\n",
    "    print('clip_grad_norm works perfectly')\n",
    "    \n",
    "    \n",
    "unit_tests_clip_grad_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "155375b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0929275],\n",
       "       [1.5605981]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(2, 4).astype('float32')\n",
    "w = np.random.rand(4, 1).astype('float32')\n",
    "b =  np.random.rand(1).astype('float32')\n",
    "x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9927bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_data(num_samples=500, in_features=4, out_features=1):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    X = np.random.rand(num_samples, in_features).astype('float32')\n",
    "    true_weights = np.random.rand(in_features, out_features).astype('float32')\n",
    "    true_bias = np.random.rand(out_features).astype('float32')\n",
    "    y = X @ true_weights + true_bias \n",
    "    noise = np.random.normal(0, 0.1, size=(num_samples, out_features)).astype('float32') \n",
    "    y += noise\n",
    "    \n",
    "    #* convert to tensors\n",
    "    X = Tensor(X, requires_grad=False)\n",
    "    y = Tensor(y, requires_grad=False) \n",
    "    return X, y\n",
    "\n",
    "X,y = generate_linear_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e61266e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 4), (500, 1))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1157535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#* define the hyperparameters\n",
    "MAX_EPOCHS = 1000\n",
    "MOMENTUM = 0.0\n",
    "WEIGHT_DECAY = 0.0\n",
    "MAX_LR = 0.01\n",
    "MIN_LR = 0.001\n",
    "BATCH_SIZE = 32\n",
    "STEPS = MAX_EPOCHS / 10\n",
    "ACCUMULATION_STEPS = 1\n",
    "\n",
    "def train_model(Optimizerclass,\n",
    "                optimizer_name,\n",
    "                max_iters=200,\n",
    "                lr=0.05,\n",
    "                num_samples=500,\n",
    "                in_features=4,\n",
    "                out_features=1):\n",
    "    print(f'\\nTraining with {optimizer_name}')\n",
    "    STEPS = max_iters / 10\n",
    "    \n",
    "    # get the data\n",
    "    X, y = generate_linear_data(num_samples= num_samples, in_features=in_features)\n",
    "    \n",
    "    # instantiate the model and the optimizer\n",
    "    model = Linear(in_features= in_features, out_features= out_features)\n",
    "    optimizer = Optimizerclass(model.parameters(), lr=lr)\n",
    "    loss_fn = MSE()\n",
    "    \n",
    "    #* get the dataloader\n",
    "    ds = TensorDataset(X,y)\n",
    "    dataloader = DataLoader(dataset=ds, batch_size=8, shuffle=True)\n",
    "    \n",
    "    #* get the trainer\n",
    "    train = Trainer(\n",
    "        model=model,\n",
    "        optimizer= optimizer,\n",
    "        loss_fn= loss_fn,\n",
    "        clip_gradients= True\n",
    "    )\n",
    "    \n",
    "    # training loop\n",
    "    for iteration in range(max_iters):\n",
    "        loss = train.train_epoch(dataloader)\n",
    "        \n",
    "        if (iteration + 1) % STEPS == 0:\n",
    "            print(f'Training Epoch: {iteration + 1} | loss: {loss}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "342b1b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with SGD Optimizer\n",
      "Training Epoch: 2 | loss: 0.16034837803546995\n",
      "Training Epoch: 4 | loss: 0.04895071983956229\n",
      "Training Epoch: 6 | loss: 0.03679352021414672\n",
      "Training Epoch: 8 | loss: 0.029322069325587254\n",
      "Training Epoch: 10 | loss: 0.02453618439956383\n",
      "Training Epoch: 12 | loss: 0.02144237129418736\n",
      "Training Epoch: 14 | loss: 0.01977832196577257\n",
      "Training Epoch: 16 | loss: 0.01804137023251128\n",
      "Training Epoch: 18 | loss: 0.017512256983210718\n",
      "Training Epoch: 20 | loss: 0.01684939786655444\n"
     ]
    }
   ],
   "source": [
    "train_model(SGD,optimizer_name='SGD Optimizer', lr= 0.01, max_iters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cab1b8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer_state': {}, 'scheduler_state': None}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16002a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.nn.layers import (Linear,\n",
    "                                Dropout,\n",
    "                                Sequential,\n",
    "                                Residual,\n",
    "                                BatchNormalization,\n",
    "                                LayerNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a594b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[ 1.78155558e+00 -2.06984730e+00  7.47858157e-01 -2.60976894e-01\n",
      "  -1.38405754e+00  1.31465025e+00]\n",
      " [-1.47097883e-01  2.14242717e+00 -1.70399194e+00  2.04098982e-01\n",
      "  -7.96782925e-01  1.11450121e+00]\n",
      " [ 2.04086384e-03  8.88168716e-02 -4.28728900e-01  5.79349621e-01\n",
      "   5.02843628e-01  5.46330184e-01]\n",
      " [-7.72783475e-01 -1.99241975e-02  1.15874036e+00 -1.73275252e-01\n",
      "   3.68702551e-01  7.98850582e-01]\n",
      " [ 4.02954116e-01 -9.31626478e-01 -1.14706620e+00 -5.11614065e-01\n",
      "   3.79190435e-01  1.38519804e-01]])\n"
     ]
    }
   ],
   "source": [
    "batch, in_features, out_features = 5,6,5\n",
    "layer1 = Linear(in_features, out_features, bias=True)\n",
    "layer2 = Linear(out_features, in_features, bias=True)\n",
    "layer3 = Linear(in_features, 4, bias=True)\n",
    "\n",
    "drop_out = Dropout(0.2, training=True)\n",
    "\n",
    "d_model = Sequential([layer1,drop_out, layer2,drop_out, layer3])\n",
    "nd_model = Sequential([layer1, layer2, layer3])\n",
    "\n",
    "x = Tensor(np.random.randn(batch, in_features))\n",
    "print(x)\n",
    "d_out = d_model(x)\n",
    "nd_out = nd_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28655154",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = layer1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa199e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(in_features=6, out_features=5, bias=True)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870ed5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(num_features=d_out.shape[-1])\n",
    "out_norm = bn(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcc143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LayerNormalization(num_features=d_out.shape[-1])\n",
    "out_lnorm = lm(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044171eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[ 1.12492785  0.54550954 -1.20440288  1.00493992]\n",
       "  [-1.60684799  0.69069154  1.58056824 -0.84682306]\n",
       "  [-0.01965291 -1.68024021 -0.13631263 -1.23028523]\n",
       "  [ 0.36564721 -0.16662922 -0.16299679  0.2331118 ]\n",
       "  [ 0.13592584  0.61066835 -0.07685594  0.83905658]], shape=(5, 4), grad_info= requires_grad=True),\n",
       " Tensor(data=[[ 1.43410922 -0.33948102 -0.88565259 -0.2089756 ]\n",
       "  [-1.41740584  0.3394242   0.92286251  0.15511913]\n",
       "  [ 0.73052329 -0.99377602  0.98110735 -0.71785461]\n",
       "  [ 1.48172841 -0.7114378  -0.35928526 -0.41100535]\n",
       "  [ 1.35740922 -0.87382482 -0.61094813  0.12736374]], shape=(5, 4), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_norm, out_lnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a059cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Linear layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n",
      "ðŸ§ª Xavier initialization tests passed ...\n",
      "ðŸ§ª Bias initialization tests passed ...\n",
      "ðŸ§ª Forward pass tests passed ...\n",
      "âœ… All unit tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "TOLERANCE = 1e-3\n",
    "\n",
    "def unit_test():\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Linear layer...\")\n",
    "    in_features, out_features = 784, 256\n",
    "    layer = Linear(in_features, out_features, bias=True)\n",
    "    \n",
    "    #* test layer creation\n",
    "    assert layer.weight.shape == (out_features, in_features), \"Weight shape is incorrect.\"\n",
    "    assert layer.bias.shape == (out_features,), \"Bias shape is incorrect.\"\n",
    "    assert layer.weight.requires_grad, \"Weight should require gradients.\"\n",
    "    assert layer.bias.requires_grad, \"Bias should require gradients.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test Xavier initialization\n",
    "    weight_std = np.std(layer.weight.data)\n",
    "    expected_std = (1.0 / in_features) ** 0.5\n",
    "    assert np.isclose(weight_std, expected_std, atol=TOLERANCE), \"Weight initialization is not Xavier.\"\n",
    "    print(\"ðŸ§ª Xavier initialization tests passed ...\")\n",
    "    \n",
    "    #* bias tests\n",
    "    assert np.all(layer.bias.data == 0), \"Bias should be initialized to zeros.\"\n",
    "    layer_no_bias = Linear(in_features, out_features, bias=False)\n",
    "    assert layer_no_bias.bias is None, \"Bias should be None when bias=False.\"\n",
    "    params = layer_no_bias.parameters()\n",
    "    assert len(params) == 1 and params[0] is layer_no_bias.weight, \"Parameters should only include weight when bias=False.\"\n",
    "    print(\"ðŸ§ª Bias initialization tests passed ...\")\n",
    "    \n",
    "    #* forward pass tests\n",
    "    batch_size = 10\n",
    "    x = Tensor(np.random.randn(batch_size, in_features))\n",
    "    output = layer(x)\n",
    "    assert output.shape == (batch_size, out_features), f\"expected output shape {(batch_size, out_features)}, got {output.shape}.\"\n",
    "    print(\"ðŸ§ª Forward pass tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ecf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "ðŸ§ª Single sample handled correctly...\n",
      "ðŸ§ª Empty batch handled correctly...\n",
      "ðŸ§ª Numerical stability with large weights handled correctly...\n",
      "ðŸ§ª No bias case handled correctly...\n",
      "âœ… Edge cases handled correctly!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_edge_cases_linear():\n",
    "    \"\"\"ðŸ”¬ Test Linear layer edge cases.\"\"\"\n",
    "    print(\"ðŸ”¬ Edge Case Tests: Linear Layer...\")\n",
    "\n",
    "    layer = Linear(10, 5)\n",
    "\n",
    "    # Test single sample (should handle 2D input)\n",
    "    x_2d = Tensor(np.random.randn(1, 10))\n",
    "    y = layer.forward(x_2d)\n",
    "    assert y.shape == (1, 5), \"Should handle single sample\"\n",
    "    print(\"ðŸ§ª Single sample handled correctly...\")\n",
    "    \n",
    "    # Test zero batch size (edge case)\n",
    "    x_empty = Tensor(np.random.randn(0, 10))\n",
    "    y_empty = layer.forward(x_empty)\n",
    "    assert y_empty.shape == (0, 5), \"Should handle empty batch\"\n",
    "    print(\"ðŸ§ª Empty batch handled correctly...\")\n",
    "    \n",
    "    # Test numerical stability with large weights\n",
    "    layer_large = Linear(10, 5)\n",
    "    layer_large.weight.data = np.ones((5,10)) * 100  # Large but not extreme\n",
    "    x = Tensor(np.ones((1, 10)))\n",
    "    y = layer_large.forward(x)\n",
    "    assert not np.any(np.isnan(y.data)), \"Should not produce NaN with large weights\"\n",
    "    assert not np.any(np.isinf(y.data)), \"Should not produce Inf with large weights\"\n",
    "    print(\"ðŸ§ª Numerical stability with large weights handled correctly...\")\n",
    "    \n",
    "    # Test with no bias\n",
    "    layer_no_bias = Linear(10, 5, bias=False)\n",
    "    x = Tensor(np.random.randn(4, 10))\n",
    "    y = layer_no_bias.forward(x)\n",
    "    assert y.shape == (4, 5), \"Should work without bias\"\n",
    "    print(\"ðŸ§ª No bias case handled correctly...\")\n",
    "    print(\"âœ… Edge cases handled correctly!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_edge_cases_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123c81e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Dropout layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dropout did not drop the expected number of elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… All unit tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtest_unit_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtest_unit_dropout\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m dropped_elements \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(output\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m expected_dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_size \u001b[38;5;241m*\u001b[39m features \u001b[38;5;241m*\u001b[39m p)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(dropped_elements \u001b[38;5;241m-\u001b[39m expected_dropped) \u001b[38;5;241m<\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m features \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout did not drop the expected number of elements.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§ª Forward pass in training mode tests passed ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#* test forward pass in evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Dropout did not drop the expected number of elements."
     ]
    }
   ],
   "source": [
    "def test_unit_dropout():\n",
    "    \"\"\"ðŸ§ª Unit tests for Dropout layer.\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Dropout layer...\")\n",
    "    \n",
    "    #* test layer creation\n",
    "    p = 0.3\n",
    "    layer = Dropout(p)\n",
    "    assert layer.p == p, \"Dropout probability not set correctly.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in training mode\n",
    "    batch_size, features = 10, 5\n",
    "    x = Tensor(np.ones((batch_size, features)))\n",
    "    output = layer(x)\n",
    "    dropped_elements = np.sum(output.data == 0)\n",
    "    expected_dropped = int(batch_size * features * p)\n",
    "    assert abs(dropped_elements - expected_dropped) < batch_size * features * 0.1, \"Dropout did not drop the expected number of elements.\"\n",
    "    print(\"ðŸ§ª Forward pass in training mode tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in evaluation mode\n",
    "    output_eval = layer(x, training=False)\n",
    "    assert np.all(output_eval.data == x.data), \"Dropout should not modify input in evaluation mode.\"\n",
    "    print(\"ðŸ§ª Forward pass in evaluation mode tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=0 (no dropout)\n",
    "    layer_no_dropout = Dropout(0.0)\n",
    "    output_no_dropout = layer_no_dropout(x)\n",
    "    assert np.all(output_no_dropout.data == x.data), \"Dropout with p=0 should not modify input.\"\n",
    "    print(\"ðŸ§ª No dropout (p=0) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=1 (all dropped)\n",
    "    layer_all_dropout = Dropout(1.0 - 1e-6) # avoid exact 1.0 to prevent division by zero\n",
    "    output_all_dropout = layer_all_dropout(x)\n",
    "    assert np.all(output_all_dropout.data == 0), \"Dropout with p=1 should drop all elements.\"\n",
    "    print(\"ðŸ§ª All dropout (p=1) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with partial dropout\n",
    "    #* this probabilistic test may fail occasionally due to randomness\n",
    "    #* so we test statistical properties instead of exact values\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    x_large = Tensor(np.ones((1000,))) # large input for statistical test\n",
    "    layer_partial_dropout = Dropout(0.5)\n",
    "    y_train = layer_partial_dropout(x_large, training=True)\n",
    "    \n",
    "    #* count non-dropped elements(approx 50% should remain)\n",
    "    non_zero_count = np.count_nonzero(y_train.data)\n",
    "    expected = 500\n",
    "    #* Use 3-sigma bounds: std = sqrt(n*p*(1-p)) = sqrt(1000*0.5*0.5) â‰ˆ 15.8\n",
    "    std_error = np.sqrt(1000 * 0.5 * 0.5)\n",
    "    lower_bound = expected - 3 * std_error  # â‰ˆ 453\n",
    "    upper_bound = expected + 3 * std_error  # â‰ˆ 547\n",
    "    assert lower_bound < non_zero_count < upper_bound, \\\n",
    "        f\"Expected {expected}Â±{3*std_error:.0f} survivors, got {non_zero_count}\"\n",
    "    print(\"ðŸ§ª Partial dropout tests passed ...\")\n",
    "    \n",
    "    # Test scaling (surviving elements should be scaled by 1/(1-p) = 2.0)\n",
    "    surviving_values = y_train.data[y_train.data != 0]\n",
    "    expected_value = 2.0  # 1.0 / (1 - 0.5)\n",
    "    assert np.allclose(surviving_values, expected_value), f\"Surviving values should be {expected_value}\"\n",
    "    print(\"ðŸ§ª Dropout scaling tests passed ...\")\n",
    "    \n",
    "    print(\"ðŸ§ª Dropout layer tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_unit_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d374ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Linear Layer Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_linear_forward_shape():\n",
    "    layer = Linear(4, 3)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = layer(x)\n",
    "\n",
    "    assert out.shape == (5, 3)\n",
    "\n",
    "\n",
    "def test_linear_backward_pass():\n",
    "    layer = Linear(4, 2, bias=True)\n",
    "    x = Tensor(np.random.randn(6, 4), requires_grad=True)\n",
    "\n",
    "    out = layer(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert layer.weight.grad is not None\n",
    "    assert layer.bias.grad is not None\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Dropout Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_dropout_training_changes_values():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.train()\n",
    "\n",
    "    x = Tensor(np.ones((100, 10)))\n",
    "    out = dropout(x)\n",
    "\n",
    "    # Expect some zeros during training\n",
    "    assert (out.data == 0).any()\n",
    "\n",
    "\n",
    "def test_dropout_eval_is_identity():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.eval()\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 5))\n",
    "    out = dropout(x)\n",
    "\n",
    "    np.testing.assert_allclose(out.data, x.data)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Sequential Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_sequential_forward_chain():\n",
    "    model = Sequential(\n",
    "        Linear(4, 6),\n",
    "        Linear(6, 2),\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(8, 4))\n",
    "    out = model(x)\n",
    "\n",
    "    assert out.shape == (8, 2)\n",
    "\n",
    "\n",
    "def test_sequential_backward_pass():\n",
    "    model = Sequential(\n",
    "        Linear(3, 5),\n",
    "        Linear(5, 1),\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(7, 3), requires_grad=True)\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Batch Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_batchnorm_forward_shape():\n",
    "    bn = BatchNormalization(num_features=4)\n",
    "    x = Tensor(np.random.randn(10, 4))\n",
    "\n",
    "    out = bn(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "\n",
    "\n",
    "def test_batchnorm_running_stats_update():\n",
    "    bn = BatchNormalization(num_features=3)\n",
    "    bn.train()\n",
    "\n",
    "    x = Tensor(np.random.randn(20, 3))\n",
    "\n",
    "    running_mean_before = bn.running_mean.data.copy()\n",
    "    _ = bn(x)\n",
    "\n",
    "    assert not np.allclose(running_mean_before, bn.running_mean.data)\n",
    "\n",
    "\n",
    "def test_batchnorm_eval_uses_running_stats():\n",
    "    bn = BatchNormalization(num_features=2)\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 2))\n",
    "    _ = bn(x)  # update stats\n",
    "\n",
    "    bn.eval()\n",
    "    out1 = bn(x)\n",
    "    out2 = bn(x)\n",
    "\n",
    "    np.testing.assert_allclose(out1.data, out2.data)\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Layer Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_layernorm_forward_shape():\n",
    "    ln = LayerNormalization(4)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "\n",
    "\n",
    "def test_layernorm_zero_mean_unit_var():\n",
    "    ln = LayerNormalization(6)\n",
    "    x = Tensor(np.random.randn(12, 6))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    mean = out.data.mean(axis=-1)\n",
    "    var = out.data.var(axis=-1)\n",
    "\n",
    "    np.testing.assert_allclose(mean, np.zeros_like(mean), atol=1e-5)\n",
    "    np.testing.assert_allclose(var, np.ones_like(var), atol=1e-4)\n",
    "\n",
    "\n",
    "def test_layernorm_backward_pass():\n",
    "    ln = LayerNormalization(3)\n",
    "    x = Tensor(np.random.randn(4, 3), requires_grad=True)\n",
    "\n",
    "    out = ln(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert ln.weight.grad is not None\n",
    "    assert ln.bias.grad is not None\n",
    "\n",
    "test_linear_forward_shape()\n",
    "test_linear_backward_pass()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

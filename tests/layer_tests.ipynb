{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16002a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.nn.layers import (Linear,\n",
    "                                Dropout,\n",
    "                                Sequential,\n",
    "                                Residual,\n",
    "                                BatchNormalization,\n",
    "                                LayerNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a594b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[[ 0.60861002  0.02777523 -2.50207599  0.02443978  0.15802738 -0.32984906]\n",
      " [ 0.62397222 -0.43682316  0.89401898 -0.5977046  -0.47511292  0.30087595]\n",
      " [-0.67911549 -2.15712305 -0.22901759  0.87133232  0.51409134  1.43558754]\n",
      " [ 1.23051082 -0.5773788  -0.77971186  0.69407013 -0.02735502 -1.03969185]\n",
      " [-0.27782297  1.04613403  0.36037149 -1.43293299  1.42400674 -0.93866863]])\n"
     ]
    }
   ],
   "source": [
    "batch, in_features, out_features = 5,6,5\n",
    "layer1 = Linear(in_features, out_features, bias=True)\n",
    "layer2 = Linear(out_features, in_features, bias=True)\n",
    "layer3 = Linear(in_features, 4, bias=True)\n",
    "\n",
    "drop_out = Dropout(0.2, training=True)\n",
    "\n",
    "d_model = Sequential([layer1,drop_out, layer2,drop_out, layer3])\n",
    "nd_model = Sequential([layer1, layer2, layer3])\n",
    "\n",
    "x = Tensor(np.random.randn(batch, in_features))\n",
    "print(x)\n",
    "d_out = d_model(x)\n",
    "nd_out = nd_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28655154",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, bias = layer1.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa199e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linear(in_features=6, out_features=5, bias=True)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "870ed5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormalization(num_features=d_out.shape[-1])\n",
    "out_norm = bn(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcc143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LayerNormalization(num_features=d_out.shape[-1])\n",
    "out_lnorm = lm(d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044171eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[-0.42215618 -1.14602737 -1.23274619 -0.4452985 ]\n",
       "  [ 0.38421401 -0.16490078 -0.55692575 -0.69299184]\n",
       "  [-0.34259789 -0.24803059  0.46104468 -0.92649303]\n",
       "  [-1.12959271 -0.05200027 -0.06879217  0.62544728]\n",
       "  [ 1.51013276  1.610959    1.39741944  1.43933608]], shape=(5, 4), grad_info= requires_grad=True),\n",
       " Tensor(data=[[ 0.89379028 -1.32884732 -0.19516003  0.63021707]\n",
       "  [ 1.47424078 -0.3969503  -0.74901706 -0.32827342]\n",
       "  [ 0.60876542 -0.92207159  1.08666303 -0.77335686]\n",
       "  [-0.74682144 -0.41315698 -0.31393335  1.47391177]\n",
       "  [-0.50151271  1.493448   -0.38175685 -0.61017844]], shape=(5, 4), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_norm, out_lnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a059cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Linear layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n",
      "ðŸ§ª Xavier initialization tests passed ...\n",
      "ðŸ§ª Bias initialization tests passed ...\n",
      "ðŸ§ª Forward pass tests passed ...\n",
      "âœ… All unit tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "TOLERANCE = 1e-3\n",
    "\n",
    "def unit_test():\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Linear layer...\")\n",
    "    in_features, out_features = 784, 256\n",
    "    layer = Linear(in_features, out_features, bias=True)\n",
    "    \n",
    "    #* test layer creation\n",
    "    assert layer.weight.shape == (out_features, in_features), \"Weight shape is incorrect.\"\n",
    "    assert layer.bias.shape == (out_features,), \"Bias shape is incorrect.\"\n",
    "    assert layer.weight.requires_grad, \"Weight should require gradients.\"\n",
    "    assert layer.bias.requires_grad, \"Bias should require gradients.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test Xavier initialization\n",
    "    weight_std = np.std(layer.weight.data)\n",
    "    expected_std = (1.0 / in_features) ** 0.5\n",
    "    assert np.isclose(weight_std, expected_std, atol=TOLERANCE), \"Weight initialization is not Xavier.\"\n",
    "    print(\"ðŸ§ª Xavier initialization tests passed ...\")\n",
    "    \n",
    "    #* bias tests\n",
    "    assert np.all(layer.bias.data == 0), \"Bias should be initialized to zeros.\"\n",
    "    layer_no_bias = Linear(in_features, out_features, bias=False)\n",
    "    assert layer_no_bias.bias is None, \"Bias should be None when bias=False.\"\n",
    "    params = layer_no_bias.parameters()\n",
    "    assert len(params) == 1 and params[0] is layer_no_bias.weight, \"Parameters should only include weight when bias=False.\"\n",
    "    print(\"ðŸ§ª Bias initialization tests passed ...\")\n",
    "    \n",
    "    #* forward pass tests\n",
    "    batch_size = 10\n",
    "    x = Tensor(np.random.randn(batch_size, in_features))\n",
    "    output = layer(x)\n",
    "    assert output.shape == (batch_size, out_features), f\"expected output shape {(batch_size, out_features)}, got {output.shape}.\"\n",
    "    print(\"ðŸ§ª Forward pass tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ecf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "ðŸ§ª Single sample handled correctly...\n",
      "ðŸ§ª Empty batch handled correctly...\n",
      "ðŸ§ª Numerical stability with large weights handled correctly...\n",
      "ðŸ§ª No bias case handled correctly...\n",
      "âœ… Edge cases handled correctly!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_edge_cases_linear():\n",
    "    \"\"\"ðŸ”¬ Test Linear layer edge cases.\"\"\"\n",
    "    print(\"ðŸ”¬ Edge Case Tests: Linear Layer...\")\n",
    "\n",
    "    layer = Linear(10, 5)\n",
    "\n",
    "    # Test single sample (should handle 2D input)\n",
    "    x_2d = Tensor(np.random.randn(1, 10))\n",
    "    y = layer.forward(x_2d)\n",
    "    assert y.shape == (1, 5), \"Should handle single sample\"\n",
    "    print(\"ðŸ§ª Single sample handled correctly...\")\n",
    "    \n",
    "    # Test zero batch size (edge case)\n",
    "    x_empty = Tensor(np.random.randn(0, 10))\n",
    "    y_empty = layer.forward(x_empty)\n",
    "    assert y_empty.shape == (0, 5), \"Should handle empty batch\"\n",
    "    print(\"ðŸ§ª Empty batch handled correctly...\")\n",
    "    \n",
    "    # Test numerical stability with large weights\n",
    "    layer_large = Linear(10, 5)\n",
    "    layer_large.weight.data = np.ones((5,10)) * 100  # Large but not extreme\n",
    "    x = Tensor(np.ones((1, 10)))\n",
    "    y = layer_large.forward(x)\n",
    "    assert not np.any(np.isnan(y.data)), \"Should not produce NaN with large weights\"\n",
    "    assert not np.any(np.isinf(y.data)), \"Should not produce Inf with large weights\"\n",
    "    print(\"ðŸ§ª Numerical stability with large weights handled correctly...\")\n",
    "    \n",
    "    # Test with no bias\n",
    "    layer_no_bias = Linear(10, 5, bias=False)\n",
    "    x = Tensor(np.random.randn(4, 10))\n",
    "    y = layer_no_bias.forward(x)\n",
    "    assert y.shape == (4, 5), \"Should work without bias\"\n",
    "    print(\"ðŸ§ª No bias case handled correctly...\")\n",
    "    print(\"âœ… Edge cases handled correctly!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_edge_cases_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "123c81e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Dropout layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dropout did not drop the expected number of elements.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… All unit tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[43mtest_unit_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtest_unit_dropout\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m dropped_elements \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(output\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m expected_dropped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(batch_size \u001b[38;5;241m*\u001b[39m features \u001b[38;5;241m*\u001b[39m p)\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(dropped_elements \u001b[38;5;241m-\u001b[39m expected_dropped) \u001b[38;5;241m<\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m features \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDropout did not drop the expected number of elements.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ§ª Forward pass in training mode tests passed ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#* test forward pass in evaluation mode\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Dropout did not drop the expected number of elements."
     ]
    }
   ],
   "source": [
    "def test_unit_dropout():\n",
    "    \"\"\"ðŸ§ª Unit tests for Dropout layer.\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Dropout layer...\")\n",
    "    \n",
    "    #* test layer creation\n",
    "    p = 0.3\n",
    "    layer = Dropout(p)\n",
    "    assert layer.p == p, \"Dropout probability not set correctly.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in training mode\n",
    "    batch_size, features = 10, 5\n",
    "    x = Tensor(np.ones((batch_size, features)))\n",
    "    output = layer(x)\n",
    "    dropped_elements = np.sum(output.data == 0)\n",
    "    expected_dropped = int(batch_size * features * p)\n",
    "    assert abs(dropped_elements - expected_dropped) < batch_size * features * 0.1, \"Dropout did not drop the expected number of elements.\"\n",
    "    print(\"ðŸ§ª Forward pass in training mode tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in evaluation mode\n",
    "    output_eval = layer(x, training=False)\n",
    "    assert np.all(output_eval.data == x.data), \"Dropout should not modify input in evaluation mode.\"\n",
    "    print(\"ðŸ§ª Forward pass in evaluation mode tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=0 (no dropout)\n",
    "    layer_no_dropout = Dropout(0.0)\n",
    "    output_no_dropout = layer_no_dropout(x)\n",
    "    assert np.all(output_no_dropout.data == x.data), \"Dropout with p=0 should not modify input.\"\n",
    "    print(\"ðŸ§ª No dropout (p=0) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=1 (all dropped)\n",
    "    layer_all_dropout = Dropout(1.0 - 1e-6) # avoid exact 1.0 to prevent division by zero\n",
    "    output_all_dropout = layer_all_dropout(x)\n",
    "    assert np.all(output_all_dropout.data == 0), \"Dropout with p=1 should drop all elements.\"\n",
    "    print(\"ðŸ§ª All dropout (p=1) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with partial dropout\n",
    "    #* this probabilistic test may fail occasionally due to randomness\n",
    "    #* so we test statistical properties instead of exact values\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    x_large = Tensor(np.ones((1000,))) # large input for statistical test\n",
    "    layer_partial_dropout = Dropout(0.5)\n",
    "    y_train = layer_partial_dropout(x_large, training=True)\n",
    "    \n",
    "    #* count non-dropped elements(approx 50% should remain)\n",
    "    non_zero_count = np.count_nonzero(y_train.data)\n",
    "    expected = 500\n",
    "    #* Use 3-sigma bounds: std = sqrt(n*p*(1-p)) = sqrt(1000*0.5*0.5) â‰ˆ 15.8\n",
    "    std_error = np.sqrt(1000 * 0.5 * 0.5)\n",
    "    lower_bound = expected - 3 * std_error  # â‰ˆ 453\n",
    "    upper_bound = expected + 3 * std_error  # â‰ˆ 547\n",
    "    assert lower_bound < non_zero_count < upper_bound, \\\n",
    "        f\"Expected {expected}Â±{3*std_error:.0f} survivors, got {non_zero_count}\"\n",
    "    print(\"ðŸ§ª Partial dropout tests passed ...\")\n",
    "    \n",
    "    # Test scaling (surviving elements should be scaled by 1/(1-p) = 2.0)\n",
    "    surviving_values = y_train.data[y_train.data != 0]\n",
    "    expected_value = 2.0  # 1.0 / (1 - 0.5)\n",
    "    assert np.allclose(surviving_values, expected_value), f\"Surviving values should be {expected_value}\"\n",
    "    print(\"ðŸ§ª Dropout scaling tests passed ...\")\n",
    "    \n",
    "    print(\"ðŸ§ª Dropout layer tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_unit_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear forward pass passed successfully\n",
      "Linear backward pass passed successfully\n",
      "Drpout evaluation identity passed succesfuly\n",
      "Sequential forward pass passed successfully\n",
      "Sequential backward pass passed\n",
      "Batch normalization test passed successfully\n",
      "Batch Normalization evaluation uses passed\n",
      "Tensor(data=[ 3.51570624e-16  5.55111512e-17  5.55111512e-16  0.00000000e+00\n",
      "  0.00000000e+00  1.29526020e-16  0.00000000e+00  0.00000000e+00\n",
      "  4.62592927e-17 -8.32667268e-17 -4.62592927e-18 -3.70074342e-17]) Tensor(data=[0.99988861 0.99998825 0.99990978 0.99998483 0.999983   0.99999048\n",
      " 0.99999197 0.99999398 0.99998196 0.99999601 0.99999675 0.99999025])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 182\u001b[0m\n\u001b[0;32m    180\u001b[0m test_batchnorm_forward_shape()\n\u001b[0;32m    181\u001b[0m test_batchnorm_eval_uses_running_stats()\n\u001b[1;32m--> 182\u001b[0m \u001b[43mtest_layernorm_zero_mean_unit_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m test_layernorm_backward_pass()\n",
      "Cell \u001b[1;32mIn[13], line 157\u001b[0m, in \u001b[0;36mtest_layernorm_zero_mean_unit_var\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m var \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mvar(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(mean, var)\n\u001b[1;32m--> 157\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m np\u001b[38;5;241m.\u001b[39mtesting\u001b[38;5;241m.\u001b[39massert_allclose(actual\u001b[38;5;241m=\u001b[39mvar\u001b[38;5;241m.\u001b[39mdata, desired\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mones_like(var))\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer normalization with zero mean and unit variance test passed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:1710\u001b[0m, in \u001b[0;36massert_allclose.<locals>.compare\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompare\u001b[39m(x, y):\n\u001b[1;32m-> 1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumeric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\babytorch\\.venv\\lib\\site-packages\\numpy\\_core\\numeric.py:2448\u001b[0m, in \u001b[0;36misclose\u001b[1;34m(a, b, rtol, atol, equal_nan)\u001b[0m\n\u001b[0;32m   2444\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(y)\n\u001b[0;32m   2446\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m errstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   2447\u001b[0m     result \u001b[38;5;241m=\u001b[39m (less_equal(\u001b[38;5;28mabs\u001b[39m(x\u001b[38;5;241m-\u001b[39my), atol \u001b[38;5;241m+\u001b[39m rtol \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mabs\u001b[39m(y))\n\u001b[1;32m-> 2448\u001b[0m               \u001b[38;5;241m&\u001b[39m \u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2449\u001b[0m               \u001b[38;5;241m|\u001b[39m (x \u001b[38;5;241m==\u001b[39m y))\n\u001b[0;32m   2450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m equal_nan:\n\u001b[0;32m   2451\u001b[0m         result \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m isnan(x) \u001b[38;5;241m&\u001b[39m isnan(y)\n",
      "\u001b[1;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Linear Layer Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_linear_forward_shape():\n",
    "    layer = Linear(4, 3)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = layer(x)\n",
    "\n",
    "    assert out.shape == (5, 3)\n",
    "    print('Linear forward pass passed successfully')\n",
    "\n",
    "\n",
    "def test_linear_backward_pass():\n",
    "    layer = Linear(4, 2, bias=True)\n",
    "    x = Tensor(np.random.randn(6, 4), requires_grad=True)\n",
    "\n",
    "    out = layer(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert layer.weight.grad is not None\n",
    "    assert layer.bias.grad is not None\n",
    "    print('Linear backward pass passed successfully')\n",
    "    \n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Dropout Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_dropout_training_changes_values():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.train()\n",
    "\n",
    "    x = Tensor(np.ones((3, 10)))\n",
    "    out = dropout(x)\n",
    "\n",
    "    # Expect some zeros during training\n",
    "    assert (out.data == 0).any(), 'There are no zeros in the tensor'\n",
    "    print('Dropout training values test passed')\n",
    "\n",
    "\n",
    "def test_dropout_eval_is_identity():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.eval()\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 5))\n",
    "    out = dropout(x)\n",
    "\n",
    "    np.testing.assert_allclose(out.data, x.data)\n",
    "    print('Drpout evaluation identity passed succesfuly')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Sequential Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_sequential_forward_chain():\n",
    "    model = Sequential([\n",
    "        Linear(4, 6),\n",
    "        Linear(6, 2),]\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(8, 4))\n",
    "    out = model(x)\n",
    "\n",
    "    assert out.shape == (8, 2)\n",
    "    print('Sequential forward pass passed successfully')\n",
    "\n",
    "def test_sequential_backward_pass():\n",
    "    model = Sequential(\n",
    "        [Linear(3, 5),\n",
    "        Linear(5, 1),]\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(7, 3), requires_grad=True)\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    print('Sequential backward pass passed')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Batch Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_batchnorm_forward_shape():\n",
    "    bn = BatchNormalization(num_features=4)\n",
    "    x = Tensor(np.random.randn(10, 4))\n",
    "\n",
    "    out = bn(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "    print('Batch normalization test passed successfully')\n",
    "\n",
    "\n",
    "def test_batchnorm_running_stats_update():\n",
    "    bn = BatchNormalization(num_features=3)\n",
    "    bn.train()\n",
    "\n",
    "    x = Tensor(np.random.randn(20, 3))\n",
    "\n",
    "    running_mean_before = bn.running_mean.data.copy()\n",
    "    _ = bn(x)\n",
    "\n",
    "    assert not np.allclose(running_mean_before, bn.running_mean.data)\n",
    "    print('Batch Normalization running stats update succesfull')\n",
    "\n",
    "\n",
    "def test_batchnorm_eval_uses_running_stats():\n",
    "    bn = BatchNormalization(num_features=2)\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 2))\n",
    "    _ = bn(x)  # update stats\n",
    "\n",
    "    bn.eval()\n",
    "    out1 = bn(x)\n",
    "    out2 = bn(x)\n",
    "\n",
    "    np.testing.assert_allclose(out1.data, out2.data) # pyright: ignore[reportCallIssue]\n",
    "    print('Batch Normalization evaluation uses passed')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Layer Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_layernorm_forward_shape():\n",
    "    ln = LayerNormalization(4)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "    print('Layer normalization forward pass passed')\n",
    "\n",
    "\n",
    "def test_layernorm_zero_mean_unit_var():\n",
    "    ln = LayerNormalization(6)\n",
    "    x = Tensor(np.random.randn(12, 6))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    mean = out.mean(axis=-1)\n",
    "    var = out.var(axis=-1)\n",
    "\n",
    "    np.testing.assert_allclose(actual=mean.data, desired=np.zeros_like(mean))\n",
    "    np.testing.assert_allclose(actual=var.data, desired=np.ones_like(var))\n",
    "    print('Layer normalization with zero mean and unit variance test passed')\n",
    "\n",
    "def test_layernorm_backward_pass():\n",
    "    ln = LayerNormalization(3)\n",
    "    x = Tensor(np.random.randn(4, 3), requires_grad=True)\n",
    "\n",
    "    out = ln(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert ln.weight.grad is not None\n",
    "    assert ln.bias.grad is not None\n",
    "    print('LayerNormalization backward test passed')\n",
    "\n",
    "test_linear_forward_shape()\n",
    "test_linear_backward_pass()\n",
    "# test_dropout_training_changes_values()\n",
    "test_dropout_eval_is_identity()\n",
    "test_sequential_forward_chain()\n",
    "test_sequential_backward_pass()\n",
    "test_batchnorm_forward_shape()\n",
    "test_batchnorm_eval_uses_running_stats()\n",
    "# test_layernorm_zero_mean_unit_var()\n",
    "test_layernorm_backward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = Tensor(np.array([1., 1., 1.,]))\n",
    "weight = Tensor(np.array([[-0.23160526,  1.09547947, -0.86387422],\n",
    " [-1.10820745 , 0.27320463,  0.83500282],\n",
    " [ 1.01400469, -0.98537308 ,-0.02863161],\n",
    " [-1.08807273  ,0.87880414,  0.20926859]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e562bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[ 0.76839474  2.09547947  0.13612578]\n",
       " [-0.10820745  1.27320463  1.83500282]\n",
       " [ 2.01400469  0.01462692  0.97136839]\n",
       " [-0.08807273  1.87880414  1.20926859]], shape=(4, 3), grad_info= None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight + bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e529d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16002a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.nn.layers import (Linear,\n",
    "                                Dropout,\n",
    "                                Sequential,\n",
    "                                Residual,\n",
    "                                BatchNormalization,\n",
    "                                LayerNormalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70a059cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Linear layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n",
      "ðŸ§ª Xavier initialization tests passed ...\n",
      "ðŸ§ª Bias initialization tests passed ...\n",
      "ðŸ§ª Forward pass tests passed ...\n",
      "âœ… All unit tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "TOLERANCE = 1e-3\n",
    "\n",
    "def unit_test():\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Linear layer...\")\n",
    "    in_features, out_features = 784, 256\n",
    "    layer = Linear(in_features, out_features, bias=True)\n",
    "    \n",
    "    #* test layer creation\n",
    "    assert layer.weight.shape == (out_features, in_features), \"Weight shape is incorrect.\"\n",
    "    assert layer.bias.shape == (out_features,), \"Bias shape is incorrect.\"\n",
    "    assert layer.weight.requires_grad, \"Weight should require gradients.\"\n",
    "    assert layer.bias.requires_grad, \"Bias should require gradients.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test Xavier initialization\n",
    "    weight_std = np.std(layer.weight.data)\n",
    "    expected_std = (1.0 / in_features) ** 0.5\n",
    "    assert np.isclose(weight_std, expected_std, atol=TOLERANCE), \"Weight initialization is not Xavier.\"\n",
    "    print(\"ðŸ§ª Xavier initialization tests passed ...\")\n",
    "    \n",
    "    #* bias tests\n",
    "    assert np.all(layer.bias.data == 0), \"Bias should be initialized to zeros.\"\n",
    "    layer_no_bias = Linear(in_features, out_features, bias=False)\n",
    "    assert layer_no_bias.bias is None, \"Bias should be None when bias=False.\"\n",
    "    params = layer_no_bias.parameters()\n",
    "    assert len(params) == 1 and params[0] is layer_no_bias.weight, \"Parameters should only include weight when bias=False.\"\n",
    "    print(\"ðŸ§ª Bias initialization tests passed ...\")\n",
    "    \n",
    "    #* forward pass tests\n",
    "    batch_size = 10\n",
    "    x = Tensor(np.random.randn(batch_size, in_features))\n",
    "    output = layer(x)\n",
    "    assert output.shape == (batch_size, out_features), f\"expected output shape {(batch_size, out_features)}, got {output.shape}.\"\n",
    "    print(\"ðŸ§ª Forward pass tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ecf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¬ Edge Case Tests: Linear Layer...\n",
      "ðŸ§ª Single sample handled correctly...\n",
      "ðŸ§ª Empty batch handled correctly...\n",
      "ðŸ§ª Numerical stability with large weights handled correctly...\n",
      "ðŸ§ª No bias case handled correctly...\n",
      "âœ… Edge cases handled correctly!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_edge_cases_linear():\n",
    "    \"\"\"ðŸ”¬ Test Linear layer edge cases.\"\"\"\n",
    "    print(\"ðŸ”¬ Edge Case Tests: Linear Layer...\")\n",
    "\n",
    "    layer = Linear(10, 5)\n",
    "\n",
    "    # Test single sample (should handle 2D input)\n",
    "    x_2d = Tensor(np.random.randn(1, 10))\n",
    "    y = layer.forward(x_2d)\n",
    "    assert y.shape == (1, 5), \"Should handle single sample\"\n",
    "    print(\"ðŸ§ª Single sample handled correctly...\")\n",
    "    \n",
    "    # Test zero batch size (edge case)\n",
    "    x_empty = Tensor(np.random.randn(0, 10))\n",
    "    y_empty = layer.forward(x_empty)\n",
    "    assert y_empty.shape == (0, 5), \"Should handle empty batch\"\n",
    "    print(\"ðŸ§ª Empty batch handled correctly...\")\n",
    "    \n",
    "    # Test numerical stability with large weights\n",
    "    layer_large = Linear(10, 5)\n",
    "    layer_large.weight.data = np.ones((5,10)) * 100  # Large but not extreme\n",
    "    x = Tensor(np.ones((1, 10)))\n",
    "    y = layer_large.forward(x)\n",
    "    assert not np.any(np.isnan(y.data)), \"Should not produce NaN with large weights\"\n",
    "    assert not np.any(np.isinf(y.data)), \"Should not produce Inf with large weights\"\n",
    "    print(\"ðŸ§ª Numerical stability with large weights handled correctly...\")\n",
    "    \n",
    "    # Test with no bias\n",
    "    layer_no_bias = Linear(10, 5, bias=False)\n",
    "    x = Tensor(np.random.randn(4, 10))\n",
    "    y = layer_no_bias.forward(x)\n",
    "    assert y.shape == (4, 5), \"Should work without bias\"\n",
    "    print(\"ðŸ§ª No bias case handled correctly...\")\n",
    "    print(\"âœ… Edge cases handled correctly!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_edge_cases_linear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "123c81e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ§ªRunning unit tests for Dropout layer...\n",
      "ðŸ§ª Layer creation tests passed ...\n",
      "ðŸ§ª Forward pass in training mode tests passed ...\n",
      "ðŸ§ª Forward pass in evaluation mode tests passed ...\n",
      "ðŸ§ª No dropout (p=0) tests passed ...\n",
      "ðŸ§ª All dropout (p=1) tests passed ...\n",
      "ðŸ§ª Partial dropout tests passed ...\n",
      "ðŸ§ª Dropout scaling tests passed ...\n",
      "ðŸ§ª Dropout layer tests passed ...\n",
      "âœ… All unit tests passed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def test_unit_dropout():\n",
    "    \"\"\"ðŸ§ª Unit tests for Dropout layer.\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"ðŸ§ªRunning unit tests for Dropout layer...\")\n",
    "    \n",
    "    #* test layer creation\n",
    "    p = 0.3\n",
    "    layer = Dropout(p)\n",
    "    assert layer.p == p, \"Dropout probability not set correctly.\"\n",
    "    print(\"ðŸ§ª Layer creation tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in training mode\n",
    "    batch_size, features = 10, 5\n",
    "    x = Tensor(np.ones((batch_size, features)))\n",
    "    output = layer(x)\n",
    "    dropped_elements = np.sum(output.data == 0)\n",
    "    expected_dropped = int(batch_size * features * p)\n",
    "    # print(abs(dropped_elements - expected_dropped))\n",
    "    assert abs(dropped_elements - expected_dropped) == batch_size * features * 0.3, \"Dropout did not drop the expected number of elements.\"\n",
    "    print(\"ðŸ§ª Forward pass in training mode tests passed ...\")\n",
    "    \n",
    "    #* test forward pass in evaluation mode\n",
    "    layer = Dropout(p, training=False)\n",
    "    output_eval = layer(x)\n",
    "    assert np.all(output_eval.data == x.data), \"Dropout should not modify input in evaluation mode.\"\n",
    "    print(\"ðŸ§ª Forward pass in evaluation mode tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=0 (no dropout)\n",
    "    layer_no_dropout = Dropout(0.0)\n",
    "   \n",
    "    output_no_dropout = layer_no_dropout(x)\n",
    "    assert np.all(output_no_dropout.data == x.data), \"Dropout with p=0 should not modify input.\"\n",
    "    print(\"ðŸ§ª No dropout (p=0) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with p=1 (all dropped)\n",
    "    layer_all_dropout = Dropout(1.0) # avoid exact 1.0 to prevent division by zero\n",
    "    x = Tensor(np.random.randint(high=1, low=0,size= (batch_size, features)))\n",
    "    output_all_dropout = layer_all_dropout(x)\n",
    "    assert np.all(output_all_dropout.data == 0), \"Dropout with p=1 should drop all elements.\"\n",
    "    print(\"ðŸ§ª All dropout (p=1) tests passed ...\")\n",
    "    \n",
    "    #* test training mode with partial dropout\n",
    "    #* this probabilistic test may fail occasionally due to randomness\n",
    "    #* so we test statistical properties instead of exact values\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    x_large = Tensor(np.ones((1000,))) # large input for statistical test\n",
    "    layer_partial_dropout = Dropout(0.5, training=True)\n",
    "    y_train = layer_partial_dropout(x_large)\n",
    "    \n",
    "    #* count non-dropped elements(approx 50% should remain)\n",
    "    non_zero_count = np.count_nonzero(y_train.data)\n",
    "    expected = 500\n",
    "    #* Use 3-sigma bounds: std = sqrt(n*p*(1-p)) = sqrt(1000*0.5*0.5) â‰ˆ 15.8\n",
    "    std_error = np.sqrt(1000 * 0.5 * 0.5)\n",
    "    lower_bound = expected - 3 * std_error  # â‰ˆ 453\n",
    "    upper_bound = expected + 3 * std_error  # â‰ˆ 547\n",
    "    assert lower_bound < non_zero_count < upper_bound, \\\n",
    "        f\"Expected {expected}Â±{3*std_error:.0f} survivors, got {non_zero_count}\"\n",
    "    print(\"ðŸ§ª Partial dropout tests passed ...\")\n",
    "    \n",
    "    # Test scaling (surviving elements should be scaled by 1/(1-p) = 2.0)\n",
    "    surviving_values = y_train.data[y_train.data != 0]\n",
    "    expected_value = 2.0  # 1.0 / (1 - 0.5)\n",
    "    assert np.allclose(surviving_values, expected_value), f\"Surviving values should be {expected_value}\"\n",
    "    print(\"ðŸ§ª Dropout scaling tests passed ...\")\n",
    "    \n",
    "    print(\"ðŸ§ª Dropout layer tests passed ...\")\n",
    "    print(\"âœ… All unit tests passed!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "test_unit_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d374ded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear forward pass passed successfully\n",
      "Linear backward pass passed successfully\n",
      "Drpout evaluation identity passed succesfuly\n",
      "Sequential forward pass passed successfully\n",
      "Sequential backward pass passed\n",
      "Batch normalization test passed successfully\n",
      "Batch Normalization evaluation uses passed\n",
      "LayerNormalization backward test passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytest\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Linear Layer Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_linear_forward_shape():\n",
    "    layer = Linear(4, 3)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = layer(x)\n",
    "\n",
    "    assert out.shape == (5, 3)\n",
    "    print('Linear forward pass passed successfully')\n",
    "\n",
    "\n",
    "def test_linear_backward_pass():\n",
    "    layer = Linear(4, 2, bias=True)\n",
    "    x = Tensor(np.random.randn(6, 4), requires_grad=True)\n",
    "\n",
    "    out = layer(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert layer.weight.grad is not None\n",
    "    assert layer.bias.grad is not None\n",
    "    print('Linear backward pass passed successfully')\n",
    "    \n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Dropout Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_dropout_training_changes_values():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.train()\n",
    "\n",
    "    x = Tensor(np.ones((3, 10)))\n",
    "    out = dropout(x)\n",
    "\n",
    "    # Expect some zeros during training\n",
    "    assert (out.data == 0).any(), 'There are no zeros in the tensor'\n",
    "    print('Dropout training values test passed')\n",
    "\n",
    "\n",
    "def test_dropout_eval_is_identity():\n",
    "    dropout = Dropout(p=0.5)\n",
    "    dropout.eval()\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 5))\n",
    "    out = dropout(x)\n",
    "\n",
    "    np.testing.assert_allclose(out.data, x.data)\n",
    "    print('Drpout evaluation identity passed succesfuly')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Sequential Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_sequential_forward_chain():\n",
    "    model = Sequential([\n",
    "        Linear(4, 6),\n",
    "        Linear(6, 2),]\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(8, 4))\n",
    "    out = model(x)\n",
    "\n",
    "    assert out.shape == (8, 2)\n",
    "    print('Sequential forward pass passed successfully')\n",
    "\n",
    "def test_sequential_backward_pass():\n",
    "    model = Sequential(\n",
    "        [Linear(3, 5),\n",
    "        Linear(5, 1),]\n",
    "    )\n",
    "\n",
    "    x = Tensor(np.random.randn(7, 3), requires_grad=True)\n",
    "    out = model(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    print('Sequential backward pass passed')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Batch Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_batchnorm_forward_shape():\n",
    "    bn = BatchNormalization(num_features=4)\n",
    "    x = Tensor(np.random.randn(10, 4))\n",
    "\n",
    "    out = bn(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "    print('Batch normalization test passed successfully')\n",
    "\n",
    "\n",
    "def test_batchnorm_running_stats_update():\n",
    "    bn = BatchNormalization(num_features=3)\n",
    "    bn.train()\n",
    "\n",
    "    x = Tensor(np.random.randn(20, 3))\n",
    "\n",
    "    running_mean_before = bn.running_mean.data.copy()\n",
    "    _ = bn(x)\n",
    "\n",
    "    assert not np.allclose(running_mean_before, bn.running_mean.data)\n",
    "    print('Batch Normalization running stats update succesfull')\n",
    "\n",
    "\n",
    "def test_batchnorm_eval_uses_running_stats():\n",
    "    bn = BatchNormalization(num_features=2)\n",
    "\n",
    "    x = Tensor(np.random.randn(10, 2))\n",
    "    _ = bn(x)  # update stats\n",
    "\n",
    "    bn.eval()\n",
    "    out1 = bn(x)\n",
    "    out2 = bn(x)\n",
    "\n",
    "    np.testing.assert_allclose(out1.data, out2.data) # pyright: ignore[reportCallIssue]\n",
    "    print('Batch Normalization evaluation uses passed')\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# Layer Normalization Tests\n",
    "# ----------------------\n",
    "\n",
    "def test_layernorm_forward_shape():\n",
    "    ln = LayerNormalization(4)\n",
    "    x = Tensor(np.random.randn(5, 4))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    assert out.shape == x.shape\n",
    "    print('Layer normalization forward pass passed')\n",
    "\n",
    "\n",
    "def test_layernorm_zero_mean_unit_var():\n",
    "    ln = LayerNormalization(6)\n",
    "    x = Tensor(np.random.randn(12, 6))\n",
    "\n",
    "    out = ln(x)\n",
    "\n",
    "    mean = out.mean(axis=-1)\n",
    "    var = out.var(axis=-1)\n",
    "\n",
    "    np.testing.assert_allclose(actual=mean.data, desired=np.zeros_like(mean))\n",
    "    np.testing.assert_allclose(actual=var.data, desired=np.ones_like(var))\n",
    "    print('Layer normalization with zero mean and unit variance test passed')\n",
    "\n",
    "def test_layernorm_backward_pass():\n",
    "    ln = LayerNormalization(3)\n",
    "    x = Tensor(np.random.randn(4, 3), requires_grad=True)\n",
    "\n",
    "    out = ln(x)\n",
    "    loss = out.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    assert x.grad is not None\n",
    "    assert ln.weight.grad is not None\n",
    "    assert ln.bias.grad is not None\n",
    "    print('LayerNormalization backward test passed')\n",
    "\n",
    "test_linear_forward_shape()\n",
    "test_linear_backward_pass()\n",
    "# test_dropout_training_changes_values()\n",
    "test_dropout_eval_is_identity()\n",
    "test_sequential_forward_chain()\n",
    "test_sequential_backward_pass()\n",
    "test_batchnorm_forward_shape()\n",
    "test_batchnorm_eval_uses_running_stats()\n",
    "# test_layernorm_zero_mean_unit_var()\n",
    "test_layernorm_backward_pass()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

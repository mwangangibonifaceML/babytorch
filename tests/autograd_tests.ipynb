{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "430a9786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8b0d920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.autograd.autograd import AddBackward, MulBackward, MatMulBackward, DivBackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ea8e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor(np.array([1.0, 2.0, 3.0]), requires_grad=True)\n",
    "b = Tensor(np.array([4.0, 5.0, 6.0]), requires_grad=True)\n",
    "gradient = Tensor(np.array([2.0, 2.0, 2.0]))\n",
    "add_backward = AddBackward(a, b)\n",
    "grad_a, grad_b = add_backward(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1e32aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mul_backward = MulBackward(a, b)\n",
    "grad_a_mul, grad_b_mul = mul_backward(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "98b8a4d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[ 8. 10. 12.], shape=(3,), grad_info= None),\n",
       " Tensor(data=[2. 4. 6.], shape=(3,), grad_info= None))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_a_mul, grad_b_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3b7987f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matmul_backward = MatMulBackward(a.reshape(3, 1), b.reshape(1, 3))\n",
    "gradient_output = Tensor(np.array([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0],[7.0, 8.0, 9.0]]))\n",
    "grad_a_matmul, grad_b_matmul = matmul_backward(gradient_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fad7c291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[ 32.]\n",
       "  [ 77.]\n",
       "  [122.]], shape=(3, 1), grad_info= requires_grad=True),\n",
       " Tensor(data=[[30. 36. 42.]], shape=(1, 3), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_a_matmul, grad_b_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "df69a89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Tensor([1.0, 2.0, 3.5], requires_grad=True)\n",
    "b = Tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "add = a + b\n",
    "sub = a - b\n",
    "mul = a * b\n",
    "div = a / b\n",
    "matmul = a.reshape(3, 1) @ b.reshape(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c2a029a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[ 32.]\n",
       "  [ 77.]\n",
       "  [122.]], shape=(3, 1), grad_info= requires_grad=True),\n",
       " Tensor(data=[[33.5 40.  46.5]], shape=(1, 3), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_backward = MatMulBackward(a.reshape(3, 1), b.reshape(1, 3))\n",
    "gradient_output = Tensor(np.array([[1.0, 2.0, 3.0],\n",
    "                                [4.0, 5.0, 6.0],\n",
    "                                [7.0, 8.0, 9.0]]))\n",
    "matmul_backward(gradient_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "db6e0a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[1. 2. 3.]\n",
       " [4. 5. 6.]\n",
       " [7. 8. 9.]], shape=(3, 3), grad_info= None)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "91c053e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[33.5 40.  46.5]], shape=(1, 3), grad_info= requires_grad=True)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.reshape(3,1).transpose() @ gradient_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6d6565a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor(data=[[ 32.]\n",
       "  [ 77.]\n",
       "  [122.]], shape=(3, 1), grad_info= requires_grad=True),\n",
       " Tensor(data=[[30. 36. 42.]], shape=(1, 3), grad_info= requires_grad=True))"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_a_matmul, grad_b_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "0b6b05e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[216], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m div_backward \u001b[38;5;241m=\u001b[39m DivBackward(a,b)\n\u001b[1;32m----> 2\u001b[0m grad_a, grad_b \u001b[38;5;241m=\u001b[39m \u001b[43mdiv_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradient_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\babytorch\\minitorch\\autograd\\autograd.py:169\u001b[0m, in \u001b[0;36mDivBackward.__call__\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grad_output, Tensor), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_output must be a Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m grad_a \u001b[38;5;241m=\u001b[39m grad_output \u001b[38;5;241m/\u001b[39m b\n\u001b[1;32m--> 169\u001b[0m grad_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mgrad_output\u001b[49m \u001b[38;5;241m*\u001b[39m a \u001b[38;5;241m/\u001b[39m (b\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__pow__\u001b[39m(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grad_a, grad_b\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'Tensor'"
     ]
    }
   ],
   "source": [
    "div_backward = DivBackward(a,b)\n",
    "grad_a, grad_b = div_backward(gradient_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "430a9786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b0d920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from minitorch.tensor.tensor import Tensor\n",
    "from minitorch.activations.activations import (\n",
    "    Softmax,\n",
    "    Sigmoid,\n",
    "    ReLU,\n",
    "    GELU,\n",
    "    Tanh\n",
    ")\n",
    "from minitorch.losses.losses import MSE, SoftMaxCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0219b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor(np.random.randn(1,5), requires_grad=True)\n",
    "weight1 = Tensor(np.random.rand(5,5), requires_grad=True)\n",
    "bias1 = Tensor(np.random.randn(5,), requires_grad=True)\n",
    "\n",
    "predicted = x1 @ weight1 + bias1\n",
    "true = Tensor(np.random.randn(1,5), requires_grad=True)\n",
    "\n",
    "loss_fn = MSE()\n",
    "loss = loss_fn(predicted, true)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3deeea66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[-1.05527259  2.3816479   0.45572236  0.46083004 -0.40211405]], shape=(1, 5), grad_info= requires_grad=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dbf5ac91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.3402532, 1.0009477, 2.0437615, 1.3149291, 1.5192282]],\n",
       "       dtype=float32),\n",
       " array([[ 0.55725867,  0.8601358 ,  1.1983076 ,  0.36606446,  0.3420838 ],\n",
       "        [-0.42049533, -0.64903986, -0.90421706, -0.27622432, -0.25812903],\n",
       "        [-0.15923941, -0.24578805, -0.34242234, -0.10460472, -0.09775213],\n",
       "        [ 0.02622831,  0.04048374,  0.05640036,  0.01722944,  0.01610075],\n",
       "        [-0.77281815, -1.1928546 , -1.6618385 , -0.50766593, -0.47440907]],\n",
       "       dtype=float32),\n",
       " array([0.44210288, 0.6823914 , 0.9506811 , 0.29041836, 0.27139324],\n",
       "       dtype=float32),\n",
       " array([[0.44210288, 0.6823914 , 0.9506811 , 0.29041836, 0.27139324]],\n",
       "       dtype=float32),\n",
       " array(1., dtype=float32))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, weight1.grad, bias1.grad, predicted.grad,loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57924c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 5), (8,))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7d23acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = Tensor(np.random.randn(8,5), requires_grad=True)\n",
    "targets = Tensor(np.random.randint(0,5, size=(8,)))\n",
    "\n",
    "loss = SoftMaxCrossEntropy()(logits, targets)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3090e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "\n",
    "class Parameter(Tensor):\n",
    "    #code \n",
    "    def __init__(self, data) -> None:\n",
    "        if isinstance(data, Tensor):\n",
    "            data = data.data # convert to raw numpy\n",
    "        else:\n",
    "            data = data\n",
    "        super().__init__(data, requires_grad=True)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        if self.grad is not None:\n",
    "            self.grad = np.zeros_like(self.data, dtype=np.float32)\n",
    "        \n",
    "    def detach(self):\n",
    "        raise RuntimeError(\n",
    "            'Cannot detach a parameter. Convert to Tensor first if intentional'\n",
    "        )\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"Parameter(Tensor(data={self.data},\\n \"\n",
    "            f\"shape={self.shape},\\n \"\n",
    "            f\"dtype={self.dtype},\\n \"\n",
    "            f\"requires_grad=True\\n \"\n",
    "            )\n",
    "\n",
    "# def _get_parameters(data):\n",
    "#     params = []\n",
    "#     if isinstance(data, Parameter):\n",
    "#         return [data] \n",
    "#     if isinstance(data, dict):\n",
    "#         for value in data.values(): #calling _get_parameters recursively\n",
    "#             if isinstance(value, Tensor):\n",
    "#                 params.extend(_get_parameters(value))\n",
    "#     if isinstance(data, (list, tuple)):\n",
    "#         for item in data:\n",
    "#             params.extend(_get_parameters(item))\n",
    "#     return params\n",
    "\n",
    "# params = _get_parameters(loss.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "23da5c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Parameter(x1)\n",
    "w1 = Parameter(weight1)\n",
    "b1 = Parameter(bias1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e2924e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=[[-1.05527259  2.3816479   0.45572236  0.46083004 -0.40211405]], shape=(1, 5), grad_info= requires_grad=True)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 @ w1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5a521fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter(Tensor(data=[[ 0.84963405 -0.32149822  0.19475245 -0.55565571  1.31958522]],\n",
       "  shape=(1, 5),\n",
       "  dtype=float64,\n",
       "  requires_grad=True\n",
       "  ,\n",
       " Parameter(Tensor(data=[[0.261551   0.56524425 0.59140681 0.37957461 0.21464685]\n",
       "  [0.48893798 0.39787141 0.38474076 0.15150718 0.65428752]\n",
       "  [0.47239929 0.38617826 0.90206245 0.2332706  0.95690201]\n",
       "  [0.48370038 0.65408458 0.88756076 0.88662014 0.61565585]\n",
       "  [0.32841528 0.61004317 0.54113322 0.99840737 0.18026287]],\n",
       "  shape=(5, 5),\n",
       "  dtype=float64,\n",
       "  requires_grad=True\n",
       "  ,\n",
       " Parameter(Tensor(data=[-1.37690453  1.5125448  -0.3196356  -0.68321827 -0.45627158],\n",
       "  shape=(5,),\n",
       "  dtype=float64,\n",
       "  requires_grad=True\n",
       "  )"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, w1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a2c7ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(data=Tensor(data=12.333215271265423), shape=(), grad_info= requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = Tensor(np.random.randn(1,5), requires_grad=True)\n",
    "weight1 = Tensor(np.random.rand(5,3), requires_grad=True)\n",
    "bias1 = Tensor(np.random.randn(3,), requires_grad=True)\n",
    "\n",
    "y1 = x1 @ weight1 + bias1\n",
    "fn1 = Softmax()\n",
    "Z1 = fn1(y1)\n",
    "Z1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d47237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor(data, requires_grad=True):\n",
    "    \"\"\"create a tensor using the Tensor class\n",
    "\n",
    "    Args:\n",
    "        data (list,ndarray): data to use to create the tensor\n",
    "        requires_grad (bool, optional): _description_. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: N-dimension tensor array storing the data\n",
    "    \"\"\"\n",
    "    return Tensor(np.array(data, dtype= np.float32), requires_grad=requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e5a80fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 1.]) Tensor(data=[1. 1.])\n",
      "Tensor(data=[0. 0.]) Tensor(data=[1. 1.])\n"
     ]
    }
   ],
   "source": [
    "def test_add_backward_basic():\n",
    "    a = tensor([1.0, 2.0])\n",
    "    b = tensor([3.0, 4.0])\n",
    "    grad_out = tensor([1.0, 1.0], requires_grad=False)\n",
    "\n",
    "    fn = AddBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    print(grad_a, grad_b)\n",
    "\n",
    "    np.testing.assert_allclose(grad_a.data, grad_out.data)\n",
    "    np.testing.assert_allclose(grad_b.data, grad_out.data)\n",
    "\n",
    "\n",
    "def test_add_backward_requires_grad_respected():\n",
    "    a = tensor([1.0, 2.0], requires_grad=False)\n",
    "    b = tensor([3.0, 4.0], requires_grad=True)\n",
    "    grad_out = tensor([1.0, 1.0], requires_grad=False)\n",
    "\n",
    "    fn = AddBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    print(grad_a, grad_b)\n",
    "    \n",
    "    test_grad = Tensor(np.zeros_like(a.data))\n",
    "    \n",
    "    np.testing.assert_allclose(grad_a.data, test_grad.data)\n",
    "    np.testing.assert_allclose(grad_b.data, grad_out.data)\n",
    "\n",
    "\n",
    "def test_add_backward_invalid_input():\n",
    "    with pytest.raises(AssertionError):\n",
    "        fn = AddBackward(1, 2)\n",
    "        fn(tensor(1.0))\n",
    "        \n",
    "test_add_backward_basic()\n",
    "test_add_backward_requires_grad_respected()\n",
    "test_add_backward_invalid_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f322a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[1. 1.]) Tensor(data=[-1. -1.])\n",
      "Tensor(data=[0. 0.]) Tensor(data=[0. 0.])\n"
     ]
    }
   ],
   "source": [
    "def test_sub_backward_basic():\n",
    "    a = tensor([1.0, 2.0])\n",
    "    b = tensor([3.0, 4.0])\n",
    "    grad_out = tensor([1.0, 1.0], requires_grad=False)\n",
    "\n",
    "    fn = SubBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    print(grad_a, grad_b)\n",
    "    \n",
    "\n",
    "    np.testing.assert_allclose(grad_a.data, grad_out.data)\n",
    "    np.testing.assert_allclose(grad_b.data, -grad_out.data)\n",
    "\n",
    "\n",
    "def test_sub_backward_requires_grad_respected():\n",
    "    a = tensor([1.0, 2.0], requires_grad=False)\n",
    "    b = tensor([3.0, 4.0], requires_grad=False)\n",
    "    grad_out = tensor([1.0, 1.0], requires_grad=False)\n",
    "\n",
    "    fn = AddBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    test_grad = Tensor(np.zeros_like(a.data))\n",
    "    print(grad_a, grad_b)\n",
    "    np.testing.assert_allclose(grad_a.data, test_grad.data)\n",
    "    np.testing.assert_allclose(grad_b.data, test_grad.data)\n",
    "\n",
    "\n",
    "def test_add_backward_invalid_input():\n",
    "    with pytest.raises(AssertionError):\n",
    "        fn = SubBackward(1, 2)\n",
    "        fn(tensor(1.0))\n",
    "   \n",
    "test_sub_backward_basic()\n",
    "test_sub_backward_requires_grad_respected()\n",
    "test_add_backward_invalid_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a17817a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(data=[10.  4.]) Tensor(data=[-30.  -8.])\n",
      "Tensor(data=[0. 0.]) Tensor(data=[-1.5 -0.5])\n"
     ]
    }
   ],
   "source": [
    "def test_div_backward_basic():\n",
    "    a = tensor([6.0, 8.0])\n",
    "    b = tensor([2.0, 4.0])\n",
    "    grad_out = tensor([20.0, 16.0], requires_grad=False)\n",
    "\n",
    "    fn = DivBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    print(grad_a, grad_b)\n",
    "    \n",
    "    np.testing.assert_allclose(\n",
    "        grad_a.data,\n",
    "        grad_out.data / b.data\n",
    "    )\n",
    "\n",
    "    np.testing.assert_allclose(\n",
    "        grad_b.data,\n",
    "        -grad_out.data * a.data / (b.data ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "def test_div_backward_requires_grad_respected():\n",
    "    a = tensor([6.0, 8.0], requires_grad=False)\n",
    "    b = tensor([2.0, 4.0], requires_grad=True)\n",
    "    grad_out = tensor([1.0, 1.0], requires_grad=False)\n",
    "\n",
    "    fn = DivBackward(a, b)\n",
    "    grad_a, grad_b = fn(grad_out)\n",
    "    test_grad = Tensor(np.zeros_like(grad_out.data))\n",
    "    print(grad_a, grad_b)\n",
    "\n",
    "    np.testing.assert_allclose(grad_a.data, test_grad.data)\n",
    "    np.testing.assert_allclose(\n",
    "        grad_b.data,\n",
    "        -grad_out.data * a.data / (b.data ** 2)\n",
    "    )\n",
    "    \n",
    "test_div_backward_basic()\n",
    "test_div_backward_requires_grad_respected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb31c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(data=[[20. 18.]\n",
      " [16. 12.]], shape=(2, 2), grad_info= None),)\n",
      "Tensor(data=[[20. 18.]\n",
      " [16. 12.]])\n"
     ]
    }
   ],
   "source": [
    "def test_transpose_backward_basic():\n",
    "    a = tensor([[6.0, 8.0], [2.0, 4.0]])\n",
    "    # b = tensor([2.0, 4.0])\n",
    "    grad_out = tensor([[20.0, 16.0], [18.0, 12.0]], requires_grad=False)\n",
    "\n",
    "    fn = TransposeBackward(a,0,1)\n",
    "    grad_a = fn(grad_out)\n",
    "    test_grad = grad_out.transpose(0,1)\n",
    "    \n",
    "    print(grad_a)\n",
    "    print(test_grad)\n",
    "    np.testing.assert_allclose(\n",
    "        grad_a[0].data,\n",
    "        test_grad.data\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "test_transpose_backward_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d8350691",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor.__init__() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgrad\n",
      "\u001b[1;31mTypeError\u001b[0m: Tensor.__init__() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "Tensor().grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_autograd(quiet=False):\n",
    "    #* GUARD: prevent double patching\n",
    "    if hasattr(Tensor, '_autograd_enabled'):\n",
    "        #* silently return the tensor if already enable - no need to warn \n",
    "        return \n",
    "    \n",
    "    #*======= STEP 1 : Add gradient infrastructure to tensor ========\n",
    "    _original_init = Tensor.__init__ # store the original init to extend it\n",
    "\n",
    "    def gradient_aware_init(self, data, requires_gradient=False):\n",
    "        \"\"\"Extend Tensor init to support gradinet tracking\"\"\"\n",
    "        _original_init(self, data)\n",
    "        self.requires_grad = requires_gradient\n",
    "        self.grad = None\n",
    "        \n",
    "    Tensor.__init__ = gradient_aware_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a13e8b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_original_init = Tensor.__init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "13daaf9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _original_init(\u001b[38;5;28;43mself\u001b[39;49m,data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "_original_init(self,data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinytorch (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
